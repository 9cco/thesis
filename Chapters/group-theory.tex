\chapter{Group Theory}

In this chapter we will introduce the minimum mathematical framework needed to
understand the phrase \say{This free energy is the $\Gamma^-_{5u}$ irreducible
representation of the $D_{4h}$ symmetry group}.

A few words about notation. We will use the semicolon `$;$' in equations as notation for the words `such that', \eg when defining sets.
A colon `$\colon$' is used when defining maps where the symbol representing the mapping itself should be on the left while the the sets
being related or how the elements of the sets are related is on the right of the colon. The colon `$:$' is also used as a shortcut for the
words `applied through its representation to' for when group elements are applied to vectors, where the correct representation to use
for this application should be implicitely understood.

\section{Discrete groups}

\section{Irreducible representations}

To know what an irreducible representation is, let's start with what we mean by a reducible representation.
\begin{defi}
    A matrix representation is \textbf{reducible} if there exists a non-trivial invariant subspace of the vector space of the representation.
\end{defi}
The intuition is then that the vector space of the representation is reducible if a ``smaller'' representation is contained within it. Since
there is a smaller vector space within the vector space of the original representation and this vector space is invariant, it is possible
to define another representation on this smaller vector space, \ie\emph{reduce} the original representation. We have now used the word
``invariant'' a couple of times, so let's define what it means more precisely.

\begin{defi}
    Let $D(g)$ a representation of the group $G$ on the vector space $V$ such that $D(g)\colon V\rightarrow V$. Then
    a subspace $U\subseteq V$ is \textbf{invariant} if 
    \begin{equation}
        \label{eq:Group:Irr:invariantDef}
        \forall g\in G\quad u\in U \implies D(g)u\in U.
    \end{equation}
\end{defi}
In other words: a vector space is invariant if it is not possible for any vector in it to escape using a representation of any group element.
All representations applied to any vector in the invariant subspace must necessarily land in that same subspace from which it started.

\begin{thm}[Shur's Lemma]
    If $D(g)$ is an irreducible complex representation with vector space $V$ and $L(V)$ is the set of all linear maps, then
    \begin{equation}
        \label{eq:Group:Irr:ShursLemma}
        \{A\in L(V) ; \; AD(g) = D(g)A\;\forall g\in G\} = \{c\mathbb{1};\; c\in\mathbb{C}\}.
    \end{equation}
\end{thm}

\section{BCS Hilbert Space}
\label{sec:Group:BCSHil}
We define the BCS Hilbert space as the Hilbert space upon which BCS-type potentials operate. Specifically this is a reduced form of the two-state
fermionic product Hilbert space $\hil_2 = \hil\otimes\hil$ where $\hil = \Span\{\ket{\v{k},s}\}$ and we only consider states that have opposite momentum.
Thus this Hilbert space is given by 
\begin{equation}
    \label{eq:Group:BCSHil:space}
    \mathcal{B} = \Span\{\ket{\v{k},s_1}\ket{-\v{k},s_2}\},
\end{equation}
and the identity operator in this space can be written
\begin{equation}
    \label{eq:Group:BCSHil:id}
    \hat{\mathbb{1}} = \sum_{\v{k}\,s_1s_2}\ket{\v{k},s_1}\ket{-\v{k},s_2}\bra{-\v{k},s_2}\bra{\v{k},s_1}.
\end{equation}
Acting on the arbitrary vector $\ket{v}\in\hilB$ with this identity operator, we find that in terms of this basis it can be written
\begin{equation}
    \label{eq:Group:BCSHil:arbitrary}
    \ket{v} = \sum_{\v{k}\,s_1s_2}v_{s_1s_2}(\v{k})\ket{\v{k},s_1}\ket{-\v{k},s_2},
\end{equation}
where
\begin{equation}
    \label{eq:Group:BCSHil:arbitrary}
    v_{s_1s_2}(\v{k}) = \bra{-\v{k},s_2}\braket{\v{k},s_1}{v}.
\end{equation}
The indices $s_1$ and $s_2$ can take on only two values each, namely $s_1,s_2\in\{\up,\dn\}$. In total there are thus $4$ different realizations
of pairs, $s_1s_2$ e.g. $\up\up$ for $v_{s_1s_2}(\v{k})$. Putting these different realizations of $v_{s_1s_2}(\v{k})$ as elements in a $2\times2$ matrix
we get
\begin{equation}
    \label{eq:Group:BCSHil:arb:mat}
    v_{s_1s_2}(\v{k}) =
    \begin{pmatrix}
        v_{\up\up}(\v{k}) & v_{\up\dn}(\v{k})\\
        v_{\dn\up}(\v{k}) & v_{\dn\dn}(\v{k})
    \end{pmatrix}.
\end{equation}
Any $2\times 2$ matrix can be written in the conventional basis of the $4$ Pauli matrices $\sigma^0 = \mathbb{1}_{2\times 2}$, $\sigma^x$, $\sigma^y$,
and $\sigma^z$. This means that we could write the matrix in Eq.~\eqref{eq:Group:BCSHil:arb:mat}
\begin{equation}
    \label{eq:GroupBCSHil:arb:pauliExp}
    v_{s_1s_2}(\v{k}) = v^0_\v{k}\sigma^0_{s_1s_2} + v^i_\v{k}\sigma^i_{s_1s_2}.
\end{equation}
It is however conventional to factor out a Pauli matrix $i\sigma^y$ to the right in the expansion since this results in nice transformation properties
of the coefficients as we shall see.
With the spin-indices expanded in this basis it is conventional to let the function of $\v{k}$
that is in front of $\sigma^0$ be called $\psi_\v{k}$. The three others are conventionally denoted $d_{\v{k},i}$. Expanded in this conventional basis
then $v_{s_1s_2}(\v{k})$ takes the form
\begin{equation}
    \label{eq:Group:BCSHil:arb:pauli}
    v_{s_1s_2}(\v{k}) = (\psi_\v{k}\sigma^0_{s_1s'} + d_{\v{k},i}\sigma^i_{s_1s'})i\sigma^y_{s's_2},
\end{equation}
and finally the state $\ket{v}$ can be written
\begin{equation}
    \label{eq:Group:BCSHil:arb:ket}
    \ket{v} = \sum_{\v{k}\,s_1s_2}[(\psi_\v{k}\sigma^0 + \v{d}_\v{k}\cdot\v{\sigma})i\sigma^y]_{s_1s_2}\ket{\v{k},s_1}\ket{-\v{k},s_2}.
\end{equation}
Going one step back and writing out the different combinations of $s_1s_2$ in $v_{s_1s_2}(\v{k})$ as a matrix like we did in
Eq.~\eqref{eq:Group:BCSHil:arb:mat}, but now multiplying out the Pauli matrices in Eq.~\eqref{eq:Group:BCSHil:arb:pauli} we get
\begin{equation}
    \label{eq:Group:BCSHil:arb:mat:Pauli}
    \begin{pmatrix}
        v_{\up\up}(\v{k}) & v_{\up\dn}(\v{k})\\
        v_{\dn\up}(\v{k}) & v_{\dn\dn}(\v{k})
    \end{pmatrix} =
    \begin{pmatrix}
        -d_{\v{k},x}+id_{\v{k},y} & \psi_\v{k}+d_{\v{k},z}\\
        -\psi_\v{k}+d_{\v{k},z} & d_{\v{k},x}+id_{\v{k},y}
    \end{pmatrix}.
\end{equation}
This set of linear relations is easily inverted to yield
\begin{align}
    \label{eq:Group:BCSHil:arb:inverseRel:psi}
    \psi_\v{k} &= \frac{1}{2}(v_{\up\dn}(\v{k}) - v_{\dn\up}(\v{k}))\\
    \label{eq:Group:BCSHil:arb:inverseRel:dx}
    d_{\v{k},x} &= \frac{1}{2}(v_{\dn\dn}(\v{k})-v_{\up\up}(\v{k}))\\
    \label{eq:Group:BCSHil:arb:inverseRel:dy}
    d_{\v{k},y} &= -\frac{i}{2}(v_{\up\up}(\v{k})+v_{\dn\dn}(\v{k}))\\
    \label{eq:Group:BCSHil:arb:inverseRel:dz}
    d_{\v{k},z} &= \frac{1}{2}(v_{\up\dn}(\v{k})+v_{\dn\up}(\v{k})).
\end{align}

Since the space $\hilB$ is fermionic we have the symmetry 
\begin{equation}
    \label{eq:Group:BCSHil:fermSymm}
    \ket{\v{k},s_1}\ket{-\v{k},s_2} = -\ket{-\v{k},s_2}\ket{\v{k},s_1}.
\end{equation}
Using this symmetry transformation on the basis vectors in the expansion of $\ket{v}$ in Eq.~\eqref{eq:Group:BCSHil:arbitrary}, then renaming indices
and finally equating coefficients term by term, we see that for the coefficients of $\ket{v}$, this symmetry takes the form
\begin{equation}
    \label{eq:GroupBCSHil:fermSymmCoeff}
    v_{s_1s_2}(\v{k}) = -v_{s_2s_1}(-\v{k}).
\end{equation}


\section{Application of group elements}

When we are talking about applying some symmetry transformation to a state, this is synonymous with applying a group element to a vector.
Even more specifically, the `applying' part means that we have some natural representation of the group on the vector space we have defined
states on, and we are using the linear transformation of the representation of the group element to act on the state vector. In this
thesis we will use the notation $g:\ket{\psi}$ to refer to this procedure.

Let $g$ be an arbitrary group element in the symmetry group $G$ and $D$ be a representation of $G$ on the $d$-dimensional vector space $V$.
Let $V$ have a basis $\{b_i\}_{i=1}^d$. The application of a group element to a basis vector is then defined as
\begin{equation}
    \label{eq:Group:Prod:basisApp}
    g:b_i = \sum_jb_jD_{ji}(g).
\end{equation}
The application of a group element to any vector in $V$ then is calculated by expanding the vector in the basis and applying the representation
$D$ to each basis vector separately as a linear transformation:
\begin{equation}
    \label{eq:Group:App:linearTransformApp}
    g:v = \sum_iv_ig:b_i.
\end{equation}

\subsection{Active vector transformation}

As $g:$ has been defined in Eq.~\eqref{eq:Group:Prod:basisApp} it is defined in a passive perspective where the transformation is happening
to the basis vectors. Applying $g$ to a vector $v$ in the basis $\{b_i\}$ it is sometimes useful to consider this as an application not
on the vectors themselves but on the expansion coefficients $v_i$ of $v$ in the basis. This is the active view of the transformation.
Inserting Eq.~\eqref{eq:Group:Prod:basisApp} into Eq.~\eqref{eq:Group:App:linearTransformApp} we get
\begin{equation}
    \label{eq:Group:App:activeTransform}
    \begin{split}
        g:v &= \sum_iv_i\sum_jb_jD_{ji}(g) = \sum_j\Big(\sum_iD_{ji}(g)v_i\Big)b_j\\
        &= \sum_iv'_ib_i,
    \end{split}
\end{equation}
where we have defined the transformed coefficients
\begin{equation}
    \label{eq:Group:App:transformedCoeff}
    v'_i = \sum_jD_{ij}(g)v_j.
\end{equation}
From this calculation we see that we can consider the application of $g$ as a transformation of the coefficients of the vector as
\begin{equation}
    \label{eq:Group:App:coeffTransformation}
    g:v_i = \sum_jD_{ij}(g)v_j.
\end{equation}
This defines the active transformation of a vector $v$ by a group element $g$.

\subsection{Representation on product spaces}

The product space $V\otimes V$ then has a basis $\{b_ib_j\}_{i,j=1}^d$. A derived representation can be constructed from $D$ on this product space
called the product representation $D^{(D\times D)}(g)$. This is defined through its application on the basis by
\begin{equation}
    \label{eq:Group:Prod:prodBasisApp}
    g:b_ib_j = \sum_{kl}b_kb_l\big[D^{(D\times D)}(g)\big]_{kl,ij} = \sum_{kl}b_kb_lD_{ki}(g)D_{lj}(g).
\end{equation}

To apply group theory to physical problems, we need to know how the objects we are working with in the physical theory transform under group elements.
The most important vector space in quantum mechanics is arguably the Hilbert space where particle states are determined by a momentum and spin
quantum number. Each quantum number has its own vector space defined by the basis vectors $\ket{\v{k}}$ and $\ket{s}$ in the Dirac notation. The
combination of both quantum numbers in the description of a particle state then gives a state in the product space of these vector spaces.
A basis for this space is given by the vectors $\ket{\v{k},s} = \ket{\v{k}}\ket{s}$. Given $\v{k}\in\mathbb{R}^d$ and $s\in\{\up,\dn\}$,
these basis vectors transform according to the product representation of the representations on each vector space, given by
\begin{equation}
    \label{eq:Group:App}
    g : \ket{\v{k}',s'} = \sum_{\v{k}s}\ket{\v{k},s}D^{(\v{k}\times s)}_{\v{k}s;\,\v{k}'s'} = \sum_{\v{k}s}\ket{\v{k},s}D_{g\;ss'}\delta_{\v{k},g:\v{k}'},
\end{equation}
under a group element $g$. Here $g\v{k}'$ means application of $g$ to the vector $\v{k}'$ through the standard representation of $g$ in $\mathbb{R}^d$.
$D_{g\;ss'}$ is a representation on the spin-up spin-down vector space given by the matrix
\begin{equation}
    \label{eq:Group:Prod:spinRepMatrix}
    D_{g\;ss'} = \sigma^0_{ss'}\cos(\phi/2) - i\hat{\v{u}}\cdot\v{\sigma}_{ss'}\sin(\phi/2),
\end{equation}
where $\hat{\v{u}}$ is the rotation axis unity vector, while $\phi$ is the angle that defines the proper rotation associated with $g$. $\v{\sigma}$ is
the vector notation for the $3$ Pauli matrices and $\sigma^0_{ss'}=\delta_{ss'}$.

In the BCS Hilbert space which we discussed in more detail in Section~\ref{sec:Group:BCSHil},
the basis vectors are outer products of the momentum spin basis vectors with opposite momentum: $\{\ket{\v{k},s_1}\ket{-\v{k},s_2}\}$. The product
representation on this vector space then transforms the basis vectors according to
\begin{equation}
    \label{eq:Group:Prod:BCSProdRep}
    g:\ket{\v{k}',s_1'}\ket{-\v{k}',s_2'} = \sum_{\v{k}\,s_1s_2}\ket{\v{k},s_1}\ket{-\v{k},s_2}D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}(g),
\end{equation}
where
\begin{equation}
    \label{eq:Group:Prod:BCSProdMatrix}
    D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}(g) = D_{g\,s_1s_1'}\delta_{\v{k},g:\v{k}'}D_{g\,s_2s_2'}\delta_{-\v{k},g:-\v{k}'}.
\end{equation}
Since group representations on $\v{k}$ is a linear transformation then $\delta_{-\v{k},g:-\v{k}'} = \delta_{\v{k},g:\v{k}'}$, such that the last
Kronecker delta function becomes superfluous.

\subsection{Representation on $\psi$-$\v{d}$ functions}

The coefficients of the basis expansion of a vector in the BCS Hilbert space were given in the conventional $\psi$-$\v{d}$ notation in 
Eq.~\eqref{eq:Group:BCSHil:arb:ket}. Taking the active view of group transformations we can say that the expansion coefficients of arbitrary states
in the BCS Hilbert space $\hilB$ transforms like the $v_i$ in Eq.~\eqref{eq:Group:App:coeffTransformation} but where now the representation
matrix $D$ is given by the matrix $D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}$ above in Eq.~\eqref{eq:Group:Prod:BCSProdMatrix}. Written
out then, the coefficients transform according to
\begin{equation}
    \label{eq:Group:pdRep:coeffTrans}
    g:v_{s_1s_2}(\v{k}) = \sum_{\v{k}'\,s_1's_2'}D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}v_{s_1's_2'}(\v{k}').
\end{equation}
Let now $\ket{v}$ be a state that is even in space, meaning that its expansion only consists of coefficients $\psi(\v{k})$ in the $\psi$-$\v{d}$
notation. Then we see from Eq.~\eqref{eq:Group:BCSHil:arb:mat:Pauli} that $\psi(\v{k})$ can be written $\psi(\v{k}) = v_{\up\dn}(\v{k})$.
The transformation properties of $\psi(\v{k})$ are thus given by
\begin{equation}
    \label{eq:Group:pdRep:psiRep}
    \begin{split}
        g:\psi(\v{k}) &= g:v_{\up\dn}(\v{k}) = \sum_{\v{k}'\,s_1's_2'}D^{(D\times D)}_{\v{k}\,\up\dn;\,\v{k}'\,s_1's_2'}\psi(\v{k}')(i\sigma^y)_{s_1's_2'}\\
        &=\psi(g^{-1}:\v{k})\big(i\sigma^y)_{\up\dn} = \psi(g^{-1}:\v{k}).
    \end{split}
\end{equation}
In this calculation we inserted the expression of $D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}$ in Eq.~\eqref{eq:Group:Prod:BCSProdMatrix} and used
the equation $D_gi\sigma^y D_g^\trans = i\sigma^y$, where $D_g$ are the spin representation matrices given in Eq.~\eqref{eq:Group:Prod:spinRepMatrix}.

To find the transformation properties of $\v{d}_\v{k}$ the principle is the same as above for $\psi(\v{k})$ but the calculations become more involved.
We assume that the spin-momentum basis expansion of state $\ket{v}$ consists of only odd coefficients so that
$v_{s_1s_2}(\v{k}) = \v{d}_\v{k}\cdot(\v{\sigma}i\sigma^y)_{s_1s_2}$. Inserting this into the active transformation of the coefficients of $\ket{V}$ in
Eq.~\eqref{eq:Group:pdRep:coeffTrans} and also inserting the expression for $D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}$ as we did before yields
\begin{equation}
    \label{eq:Group:pdRep:oddCoeffTrans}
    \begin{split}
        g:v_{s_1s_2}(\v{k}) &= \sum_{\v{k}'\,s_1's_2'}\delta_{\v{k},g:\v{k}'}D_{g\,s_1s_1'}D_{g\,s_2s_2}\v{d}_{\v{k}'}\cdot(\v{\sigma}i\sigma^y)_{s_1's_2'}\\
        &= \sum_s\Big(D_g\v{\sigma}\sigma^yD_g^\trans\sigma^y\Big)_{s_1s}\cdot\v{d}_{g^{-1}:\v{k}}i\sigma^y_{ss_2}
    \end{split}
\end{equation}

\subsection{Projection Operators}

Let us assume that we are in a vector space $V$ that can be divided into possibly several different irreducible representations $D^{(\alpha)}$ of some
symmetry group $G$. Further, let the basis vectors of these irreducible representations be denoted by $b^{(\alpha)}_m$ where $m$ thus counts the number
of basis vectors in each \irr Then an arbitrary vector $f\in V$ can be written in terms of these basis vectors as
\begin{equation}
    \label{eq:Group:Irr:Pro:basisDecomp}
    f = \sum_\alpha\sum_m c_m^{(\alpha)}b^{(\alpha)}_m.
\end{equation}

A projection operator can be used to extract any combination of constant $c_m^{(\alpha)}$ multiplied by a basis vector $b^{(\alpha)}_n$, where
$m$ and $n$ can in general be different. Denoting the projection operator that picks out the $m$th constant multiplied by the $l$th basis vector
in the \irr $\beta$ of the expansion of $f$: $P^{(\beta)}_{lm}$, then
\begin{equation}
    \label{eq:Group:Irr:Pro:generalProApplication}
    P^{(\beta)}_{lm}f = c_m^{(\beta)}b_l^{(\beta)}.
\end{equation}
This is extremely useful in finding a bases for the irreducible representations.
To acheive this, the projection operator is defined as
\begin{equation}
    \label{eq:Group:Irr:Pro:proOpDef}
    P^{(\beta)}_{l,m} = \frac{d_\beta}{\abs{G}}\sum_{g\in G}D_{lm}^{(\beta)}(g)^\ast g:,
\end{equation}
where $d_\beta$ is the dimension of \irr $\beta$, $D_{lm}^{(\beta)}(g)$ is the $lm$ element of the matrix representation of the group element $g$ and
finally we have used the notation $g:$ to denote application on vectors by the relevant representation. An example is the application of $g$ to the
basis vectors $b_m^{(\alpha)}$. Since the relevant representation of $g$ in this case is the irreducible representation for which $b_m^{(\alpha)}$ is
a basis vector, the application becomes
\begin{equation}
    \label{eq:Group:Irr:Pro:gApplication}
    g : b_m^{(\alpha)} = \sum_nb_n^{(\alpha)}D_{nm}^{(\alpha)}(g).
\end{equation}

Usually, the full generality of the
projection operators $P^{(\beta)}_{l,m}$ isn't needed and it suffices to consider the diagonal projection operators
$P^{(\beta)}_{l,l} \equiv P^{(\beta)}_l$ or indeed their sum, in which case the resulting operator can be written only in terms of the \irr characters
$\chi^{(\alpha)}(g)$ since
\begin{equation}
    \label{eq:Group:Irr:Pro:charPro}
    P^{(\beta)}\equiv\sum_lP^{(\beta)}_l = \frac{d_\beta}{\abs{G}}\sum_{g\in G}\sum_lD_{ll}^{(\beta)}(g)^\ast g: = \frac{d_\beta}{\abs{G}}\sum_{g\in G}\chi^{(\beta)}(g)^\ast g:.
\end{equation}


\section{Fermionic symmetry transformations}

\section{Time-reversal symmetry}

\section{Square Lattice Harmonics}


\section{Decomposition of the Potential}

Let at first $\hat{V}$ be a general two-body operator that acts on an $N$-particle state which is a vector in $\hil_N = \otimes_{i=1}^N\hil$. The
single particle Hilbert space $\hil$ in question is quantified by momentum and spin such that $\hil=\Span\{\ket{\v{k},s}\}$. Denoting specific combinations
of $\v{k}$ and $s$ as $\alpha$ as a shorthand for the moment, then $\hat{V}$ acts on basis vectors in $\hil_N$ as
\begin{equation}
    \label{eq:Group:Potential:twoBodyInteraction}
    \hat{V}\ket{\alpha_1}\ldots\ket{\alpha_N} = \sum_{1\leq i<j\leq N}\hat{V}_{ij}\ket{\alpha_1}\ldots\ket{\alpha_N},
\end{equation}
by definition of being a two-body operator. Here $\hat{V}_{ij}$ is an operator that only acts on the $i$th and $j$th ket. Even though $\hat{V}$ acts on
$\hil_N$, because of how it can be written in terms of $\hat{V}_{ij}$ and this only acts on two states at a time, it follows that $\hat{V}$ is
completely determined by its action on the reduced Hilbert space $\hil_2$. This implies that $\hat{V}$ is fully described by its matrix elements
\begin{equation}
    \label{eq:Group:Potential:genTwoBodyMatrixElem}
    \bra{\alpha}\bra{\alpha'}\hat{V}\ket{\beta}\ket{\beta'}.
\end{equation}
Inserting back the $\ket{\v{k},s}$ notation, these matrix elements are referred to as
\begin{equation}
    \label{eq:Group:Potential:momSpinPotentialMatrixElems}
    V_{\v{k}_1\v{k}_2\v{k}_3\v{k}_4;\,s_1s_2s_3s_4} = \bra{\v{k}_1s_1}\bra{\v{k}_2s_2}\hat{V}\ket{\v{k}_4s_4}\ket{\v{k}_3s_3}.
\end{equation}
When $\hat{V}$ is a BCS operator acting on the BCS Hilbert space described in the previous section, these matrix elements are denoted
\begin{equation}
    \label{eq:Group:Potential:bcsTwoBodyMatrixElem}
    V_{\v{k}\v{k}';\,s_1s_2s_3s_4} = \bra{\v{k}s_1}\bra{-\v{k}s_2}\hat{V}\ket{\v{k}'s_4}\ket{-\v{k}'s_3}.
\end{equation}

Since $\hat{V}$ is Hermitian, it must be diagonalizable in a basis of eigenfunctions. Barring accidental degeneracy, a basis for a $d$-degenerate
eigenvalue is also a basis for an irreducible representation of the symmetry group $G$ of the Hamiltonian. In the case of accidental degeneracy
then this $d$-dimensional vector space consists of several non-intersecting subspaces where each subspace is a basis for a (possibly different) \irr Note
that this does not mean that (barring accidental degeneracy) there exists one seperate eigenvalue for each \irr of $G$ since there might be
several different eigenvalues with different eigenspace bases but where all of them are bases for the same \irr Regardless of these details,
the connection between irreducible representations and the eigenvalues of $\hat{V}$ is a great help in finding the bases for which it is
diagonal.

We let the basis for a $d_\Gamma$-dimensional \irr $\Gamma$ be denoted $\{\ket{\Gamma,q_\Gamma,m}\}_{m=1}^{d_\Gamma}$, where $\hat{V}$ has an eigenvalue $V_{\Gamma,q_\Gamma}$
for the vectors in this basis and $q_\Gamma$ is an index enumerating the different versions of bases of $\Gamma$ that $\hat{V}$ might have
in its set of eigenspace bases. Since $\hat{V}$ then is diagonal in this set of bases then
\begin{equation}
    \label{eq:Group:Potential:potIrrepDecomp}
    \hat{V} = \sum_{\Gamma q_\Gamma}V_{\Gamma,q_\Gamma}\sum_{m=1}^{d_\Gamma}\ket{\Gamma,q_\Gamma,m}\bra{\Gamma,q_\Gamma,m}.
\end{equation}
Because of the potential for accidental degeneracy we can not guarantee that $V_{\Gamma,q_\Gamma}\neq V_{\Gamma',q_{\Gamma'}}$ for different $\Gamma$ and $\Gamma'$.
Inserting this expression for $\hat{V}$ into the matrix elements in Eq.~\eqref{eq:Group:Potential:bcsTwoBodyMatrixElem} lets us write them in terms of
irreducible representation basis vectors in the momentum spin function representation:
\begin{equation}
    \label{eq:Group:Potential:irrepPotMatrixElem}
    V_{\v{k}\v{k}';\,s_1s_2s_3s_4} = \sum_\Gamma V_{\Gamma,q_\Gamma}\sum_{m=1}^{d_\Gamma}\Psi_{s_1s_2}^{\Gamma,q_\Gamma}(\v{k})\Psi_{s_3s_4}^{\Gamma,q_\Gamma}(-\v{k}')^\dagger,
\end{equation}
where
\begin{equation}
    \label{eq:Group:Potential:irrepPotMatrixElem:coeff}
    \Psi_{s_1s_2}^{\Gamma,q_\Gamma}(\v{k}) = \bra{\v{k},s_1}\braket{-\v{k},s_2}{\Gamma,q_\Gamma,m}.
\end{equation}

We can separate the set of different irreducible representation bases into bases that have vectors that transform either symmetrically or anti-symmetrically
with respect to the group element of space inversion $P$. We call the representations of such bases even or odd representations. Even representations are those that map
$P$ to the identity operator $\mathbb{1}$ and as a consequence have $\Psi^{\Gamma,q_\Gamma,m}_{s_1s_2}(-\v{k}) = \Psi^{\Gamma,q_\Gamma,m}_{s_1s_2}(\v{k})$. Writing
the spin-indices in these functions in terms of Pauli matrices by using the expansion in Eq.~\eqref{eq:Group:BCSHil:arb:pauli} and using the fermionic symmetry then
even representations $a$ have
\begin{equation}
    \label{eq:Group:Potential:evenIrrepFunctions}
    \Psi^{a,q_a,m}_{s_1s_2}(\v{k}) = \psi^{a,q_a,m}_\v{k}i\sigma^y_{s_1s_2}.
\end{equation}
Odd representations $b$ map $P$ to the inversion operator $I$ such that $\Psi^{b,q_b,m}_{s_1s_2}(-\v{k}) = -\Psi^{b,q_b,m}_{s_1s_2}(\v{k})$. Expanding in Pauli
matrices then yields
\begin{equation}
    \label{eq:Group:Potential:oddIrrepFunctions}
    \Psi^{b,q_b,m}_{s_1s_2}(\v{k}) = \v{d}^{b,q_b,m}_{\v{k}}\cdot(\v{\sigma}i\sigma^y)_{s_1s_2}.
\end{equation}
Separating the sum over irreducible representations $\Gamma$ into sums over even ($a$) and odd ($b$) representations in the potential operator matrix elements in
Eq.~\eqref{eq:Group:Potential:irrepPotMatrixElem}, we arrive at the fully expanded expression
\begin{equation}
    \label{eq:Group:Potential:fullyExpandedPotMatrixElem}
    \begin{split}
        &V_{\v{k}\v{k}';\,s_1s_2s_3s_4} = \sum_{aq_a}V_{a,q_a}\sum_{m=1}^{d_a}\psi_\v{k}^{a,q_a,m}i\sigma^y_{s_1s_2}\big(\psi_{-\v{k}'}^{a,q_a,m}i\sigma^y_{s_3s_4}\big)^\dagger\\
        & + \sum_{bq_b}V_{b,q_b}\sum_{m=1}^{d_b}\big(\v{d}^{b,q_b,m}_{\v{k}}\cdot\v{\sigma}i\sigma^y\big)_{s_1s_2}\Big[\big(\v{d}^{b,q_b,m}_{-\v{k}'}\cdot\v{\sigma}i\sigma^y\big)_{s_3s_4}\Big]^\dagger.
    \end{split}
\end{equation}
In this use of the dagger notation, the adjoint acts on both of the matrix indices such that $\v{d}_{-\v{k}}^\dagger = \v{d}_\v{k}^\ast$ and $\sigma_{s_1s_2}^\dagger = \sigma_{s_2s_1}^\ast$.
