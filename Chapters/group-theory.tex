\chapter{Group Theory}

In this chapter we will introduce the minimum mathematical framework needed to
understand the phrase \say{This free energy is the $\Gamma^-_{5u}$ irreducible
representation of the $D_{4h}$ symmetry group}.

A few words about notation. We will use the semicolon `$;$' in equations as notation for the words `such that', \eg when defining sets.
A colon `$\colon$' is used when defining maps where the symbol representing the mapping itself should be on the left while the the sets
being related or how the elements of the sets are related is on the right of the colon. The colon `$:$' is also used as a shortcut for the
words `applied through its representation to' for when group elements are applied to vectors, where the correct representation to use
for this application should be implicitely understood.

\section{Discrete groups}

\section{Irreducible representations}

To know what an irreducible representation is, let's start with what we mean by a reducible representation.
\begin{defi}
    A matrix representation is \textbf{reducible} if there exists a non-trivial invariant subspace of the vector space of the representation.
\end{defi}
The intuition is then that the vector space of the representation is reducible if a ``smaller'' representation is contained within it. Since
there is a smaller vector space within the vector space of the original representation and this vector space is invariant, it is possible
to define another representation on this smaller vector space, \ie\emph{reduce} the original representation. We have now used the word
``invariant'' a couple of times, so let's define what it means more precisely.

\begin{defi}
    Let $D(g)$ a representation of the group $G$ on the vector space $V$ such that $D(g)\colon V\rightarrow V$. Then
    a subspace $U\subseteq V$ is \textbf{invariant} if 
    \begin{equation}
        \label{eq:Group:Irr:invariantDef}
        \forall g\in G\quad u\in U \implies D(g)u\in U.
    \end{equation}
\end{defi}
In other words: a vector space is invariant if it is not possible for any vector in it to escape using a representation of any group element.
All representations applied to any vector in the invariant subspace must necessarily land in that same subspace from which it started.

\begin{thm}[Shur's Lemma]
    If $D(g)$ is an irreducible complex representation with vector space $V$ and $L(V)$ is the set of all linear maps, then
    \begin{equation}
        \label{eq:Group:Irr:ShursLemma}
        \{A\in L(V) ; \; AD(g) = D(g)A\;\forall g\in G\} = \{c\mathbb{1};\; c\in\mathbb{C}\}.
    \end{equation}
\end{thm}

\section{BCS Hilbert Space}
\label{sec:Group:BCSHil}
We define the BCS Hilbert space as the Hilbert space upon which BCS-type potentials operate. Specifically this is a reduced form of the two-state
fermionic product Hilbert space $\hil_2 = \hil\otimes\hil$ where $\hil = \Span\{\ket{\v{k},s}\}$ and we only consider states that have opposite momentum.
Thus this Hilbert space is given by 
\begin{equation}
    \label{eq:Group:BCSHil:space}
    \mathcal{B} = \Span\{\ket{\v{k},s_1}\ket{-\v{k},s_2}\},
\end{equation}
and the identity operator in this space can be written
\begin{equation}
    \label{eq:Group:BCSHil:id}
    \hat{\mathbb{1}} = \sum_{\v{k}\,s_1s_2}\ket{\v{k},s_1}\ket{-\v{k},s_2}\bra{-\v{k},s_2}\bra{\v{k},s_1}.
\end{equation}
Acting on the arbitrary vector $\ket{v}\in\hilB$ with this identity operator, we find that in terms of this basis it can be written
\begin{equation}
    \label{eq:Group:BCSHil:arbitrary}
    \ket{v} = \sum_{\v{k}\,s_1s_2}v_{s_1s_2}(\v{k})\ket{\v{k},s_1}\ket{-\v{k},s_2},
\end{equation}
where
\begin{equation}
    \label{eq:Group:BCSHil:arbitrary}
    v_{s_1s_2}(\v{k}) = \bra{-\v{k},s_2}\braket{\v{k},s_1}{v}.
\end{equation}
The indices $s_1$ and $s_2$ can take on only two values each, namely $s_1,s_2\in\{\up,\dn\}$. In total there are thus $4$ different realizations
of pairs, $s_1s_2$ e.g. $\up\up$ for $v_{s_1s_2}(\v{k})$. Putting these different realizations of $v_{s_1s_2}(\v{k})$ as elements in a $2\times2$ matrix
we get
\begin{equation}
    \label{eq:Group:BCSHil:arb:mat}
    v_{s_1s_2}(\v{k}) =
    \begin{pmatrix}
        v_{\up\up}(\v{k}) & v_{\up\dn}(\v{k})\\
        v_{\dn\up}(\v{k}) & v_{\dn\dn}(\v{k})
    \end{pmatrix}.
\end{equation}
Any $2\times 2$ matrix can be written in the conventional basis of the $4$ Pauli matrices $\sigma^0 = \mathbb{1}_{2\times 2}$, $\sigma^x$, $\sigma^y$,
and $\sigma^z$. This means that we could write the matrix in Eq.~\eqref{eq:Group:BCSHil:arb:mat}
\begin{equation}
    \label{eq:GroupBCSHil:arb:pauliExp}
    v_{s_1s_2}(\v{k}) = v^0_\v{k}\sigma^0_{s_1s_2} + v^i_\v{k}\sigma^i_{s_1s_2}.
\end{equation}
It is however conventional to factor out a Pauli matrix $i\sigma^y$ to the right in the expansion since this results in nice transformation properties
of the coefficients as we shall see.
With the spin-indices expanded in this basis it is conventional to let the function of $\v{k}$
that is in front of $\sigma^0$ be called $\psi_\v{k}$. The three others are conventionally denoted $d_{\v{k},i}$. Expanded in this conventional basis
then $v_{s_1s_2}(\v{k})$ takes the form
\begin{equation}
    \label{eq:Group:BCSHil:arb:pauli}
    v_{s_1s_2}(\v{k}) = (\psi_\v{k}\sigma^0_{s_1s'} + d_{\v{k},i}\sigma^i_{s_1s'})i\sigma^y_{s's_2},
\end{equation}
and finally the state $\ket{v}$ can be written
\begin{equation}
    \label{eq:Group:BCSHil:arb:ket}
    \ket{v} = \sum_{\v{k}\,s_1s_2}[(\psi_\v{k}\sigma^0 + \v{d}_\v{k}\cdot\v{\sigma})i\sigma^y]_{s_1s_2}\ket{\v{k},s_1}\ket{-\v{k},s_2}.
\end{equation}
Going one step back and writing out the different combinations of $s_1s_2$ in $v_{s_1s_2}(\v{k})$ as a matrix like we did in
Eq.~\eqref{eq:Group:BCSHil:arb:mat}, but now multiplying out the Pauli matrices in Eq.~\eqref{eq:Group:BCSHil:arb:pauli} we get
\begin{equation}
    \label{eq:Group:BCSHil:arb:mat:Pauli}
    \begin{pmatrix}
        v_{\up\up}(\v{k}) & v_{\up\dn}(\v{k})\\
        v_{\dn\up}(\v{k}) & v_{\dn\dn}(\v{k})
    \end{pmatrix} =
    \begin{pmatrix}
        -d_{\v{k},x}+id_{\v{k},y} & \psi_\v{k}+d_{\v{k},z}\\
        -\psi_\v{k}+d_{\v{k},z} & d_{\v{k},x}+id_{\v{k},y}
    \end{pmatrix}.
\end{equation}
This set of linear relations is easily inverted to yield
\begin{align}
    \label{eq:Group:BCSHil:arb:inverseRel:psi}
    \psi_\v{k} &= \frac{1}{2}(v_{\up\dn}(\v{k}) - v_{\dn\up}(\v{k}))\\
    \label{eq:Group:BCSHil:arb:inverseRel:dx}
    d_{\v{k},x} &= \frac{1}{2}(v_{\dn\dn}(\v{k})-v_{\up\up}(\v{k}))\\
    \label{eq:Group:BCSHil:arb:inverseRel:dy}
    d_{\v{k},y} &= -\frac{i}{2}(v_{\up\up}(\v{k})+v_{\dn\dn}(\v{k}))\\
    \label{eq:Group:BCSHil:arb:inverseRel:dz}
    d_{\v{k},z} &= \frac{1}{2}(v_{\up\dn}(\v{k})+v_{\dn\up}(\v{k})).
\end{align}

Since the space $\hilB$ is fermionic we have the symmetry 
\begin{equation}
    \label{eq:Group:BCSHil:fermSymm}
    \ket{\v{k},s_1}\ket{-\v{k},s_2} = -\ket{-\v{k},s_2}\ket{\v{k},s_1}.
\end{equation}
Using this symmetry transformation on the basis vectors in the expansion of $\ket{v}$ in Eq.~\eqref{eq:Group:BCSHil:arbitrary}, then renaming indices
and finally equating coefficients term by term, we see that for the coefficients of $\ket{v}$, this symmetry takes the form
\begin{equation}
    \label{eq:GroupBCSHil:fermSymmCoeff}
    v_{s_1s_2}(\v{k}) = -v_{s_2s_1}(-\v{k}).
\end{equation}


\section{Application of group elements}

When we are talking about applying some symmetry transformation to a state, this is synonymous with applying a group element to a vector.
Even more specifically, the `applying' part means that we have some natural representation of the group on the vector space we have defined
states on, and we are using the linear transformation of the representation of the group element to act on the state vector. In this
thesis we will use the notation $g:\ket{\psi}$ to refer to this procedure.

Let $g$ be an arbitrary group element in the symmetry group $G$ and $D$ be a representation of $G$ on the $d$-dimensional vector space $V$.
Let $V$ have a basis $\{\v{b}_i\}_{i=1}^d$. The application of a group element to a basis vector is then defined as
\begin{equation}
    \label{eq:Group:Prod:basisApp}
    g:\v{b}_i = \sum_j\v{b}_jD_{ji}(g).
\end{equation}
The application of a group element to any vector in $V$ then is calculated by expanding the vector in the basis and applying the representation
$D$ to each basis vector separately as a linear transformation:
\begin{equation}
    \label{eq:Group:App:linearTransformApp}
    g:\v{v} = \sum_iv_ig:\v{b}_i.
\end{equation}

\subsection{Active vector transformation}

As $g:$ has been defined in Eq.~\eqref{eq:Group:Prod:basisApp} it is defined in a passive perspective where the transformation is happening
to the basis vectors. Applying $g$ to a vector $\v{v}$ in the basis $\{\v{b}_i\}$ it is sometimes useful to consider this as an application not
on the vectors themselves but on the expansion coefficients $v_i$ of $\v{v}$ in the basis. This is the active view of the transformation.
Inserting Eq.~\eqref{eq:Group:Prod:basisApp} into Eq.~\eqref{eq:Group:App:linearTransformApp} we get
\begin{equation}
    \label{eq:Group:App:activeTransform}
    \begin{split}
        g:\v{v} &= \sum_iv_i\sum_j\v{b}_jD_{ji}(g) = \sum_j\Big(\sum_iD_{ji}(g)v_i\Big)\v{b}_j\\
        &= \sum_iv'_i\v{b}_i,
    \end{split}
\end{equation}
where we have defined the transformed coefficients
\begin{equation}
    \label{eq:Group:App:transformedCoeff}
    v'_i = \sum_jD_{ij}(g)v_j.
\end{equation}
From this calculation we see that we can consider the application of $g$ as a transformation of the coefficients of the vector as
\begin{equation}
    \label{eq:Group:App:coeffTransformation}
    g:v_i = \sum_jD_{ij}(g)v_j.
\end{equation}
This defines the active transformation of a vector $v$ by a group element $g$.

\subsection{Representation on product spaces}

The product space $V\otimes V$ then has a basis $\{\v{b}_i\v{b}_j\}_{i,j=1}^d$. A derived representation can be constructed from $D$ on this product space
called the product representation $D^{(D\times D)}(g)$. This is defined through its application on the basis by
\begin{equation}
    \label{eq:Group:Prod:prodBasisApp}
    g:\v{b}_i\v{b}_j = \sum_{kl}\v{b}_k\v{b}_l\big[D^{(D\times D)}(g)\big]_{kl,ij} = \sum_{kl}\v{b}_k\v{b}_lD_{ki}(g)D_{lj}(g).
\end{equation}

To apply group theory to physical problems, we need to know how the objects we are working with in the physical theory transform under group elements.
The most important vector space in quantum mechanics is arguably the Hilbert space where particle states are determined by a momentum and spin
quantum number. Each quantum number has its own vector space defined by the basis vectors $\ket{\v{k}}$ and $\ket{s}$ in the Dirac notation. The
combination of both quantum numbers in the description of a particle state then gives a state in the product space of these vector spaces.
A basis for this space is given by the vectors $\ket{\v{k},s} = \ket{\v{k}}\ket{s}$. Given $\v{k}\in\mathbb{R}^d$ and $s\in\{\up,\dn\}$,
these basis vectors transform according to the product representation of the representations on each vector space, given by
\begin{equation}
    \label{eq:Group:App}
    g : \ket{\v{k}',s'} = \sum_{\v{k}s}\ket{\v{k},s}D^{(\v{k}\times s)}_{\v{k}s;\,\v{k}'s'} = \sum_{\v{k}s}\ket{\v{k},s}D_{g\;ss'}\delta_{\v{k},g:\v{k}'},
\end{equation}
under a group element $g$. Here $g\v{k}'$ means application of $g$ to the vector $\v{k}'$ through the standard representation of $g$ in $\mathbb{R}^d$.
$D_{g\;ss'}$ is a representation on the spin-up spin-down vector space given by the matrix
\begin{equation}
    \label{eq:Group:Prod:spinRepMatrix}
    D_{g\;ss'} = \sigma^0_{ss'}\cos(\phi/2) - i\hat{\v{u}}\cdot\v{\sigma}_{ss'}\sin(\phi/2),
\end{equation}
where $\hat{\v{u}}$ is the rotation axis unity vector, while $\phi$ is the angle that defines the proper rotation associated with $g$. $\v{\sigma}$ is
the vector notation for the $3$ Pauli matrices and $\sigma^0_{ss'}=\delta_{ss'}$.

In the BCS Hilbert space which we discussed in more detail in Section~\ref{sec:Group:BCSHil},
the basis vectors are outer products of the momentum spin basis vectors with opposite momentum: $\{\ket{\v{k},s_1}\ket{-\v{k},s_2}\}$. The product
representation on this vector space then transforms the basis vectors according to
\begin{equation}
    \label{eq:Group:Prod:BCSProdRep}
    g:\ket{\v{k}',s_1'}\ket{-\v{k}',s_2'} = \sum_{\v{k}\,s_1s_2}\ket{\v{k},s_1}\ket{-\v{k},s_2}D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}(g),
\end{equation}
where
\begin{equation}
    \label{eq:Group:Prod:BCSProdMatrix}
    D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}(g) = D_{g\,s_1s_1'}\delta_{\v{k},g:\v{k}'}D_{g\,s_2s_2'}\delta_{-\v{k},g:-\v{k}'}.
\end{equation}
Since group representations on $\v{k}$ is a linear transformation then $\delta_{-\v{k},g:-\v{k}'} = \delta_{\v{k},g:\v{k}'}$, such that the last
Kronecker delta function becomes superfluous.

\subsection{Representation on $\psi$-$\v{d}$ functions}
\label{sec:Group:pdRep}

The coefficients of the basis expansion of a vector in the BCS Hilbert space were given in the conventional $\psi$-$\v{d}$ notation in 
Eq.~\eqref{eq:Group:BCSHil:arb:ket}. Taking the active view of group transformations we can say that the expansion coefficients of arbitrary states
in the BCS Hilbert space $\hilB$ transforms like the $v_i$ in Eq.~\eqref{eq:Group:App:coeffTransformation} but where now the representation
matrix $D$ is given by the matrix $D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}$ above in Eq.~\eqref{eq:Group:Prod:BCSProdMatrix}. Written
out then, the coefficients transform according to
\begin{equation}
    \label{eq:Group:pdRep:coeffTrans}
    g:v_{s_1s_2}(\v{k}) = \sum_{\v{k}'\,s_1's_2'}D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}v_{s_1's_2'}(\v{k}').
\end{equation}
Let now $\ket{v}$ be a state that is even in space, meaning that its expansion only consists of coefficients $\psi(\v{k})$ in the $\psi$-$\v{d}$
notation. Then we see from Eq.~\eqref{eq:Group:BCSHil:arb:mat:Pauli} that $\psi(\v{k})$ can be written $\psi(\v{k}) = v_{\up\dn}(\v{k})$.
The transformation properties of $\psi(\v{k})$ are thus given by
\begin{equation}
    \label{eq:Group:pdRep:psiRep}
    \begin{split}
        g:\psi(\v{k}) &= g:v_{\up\dn}(\v{k}) = \sum_{\v{k}'\,s_1's_2'}D^{(D\times D)}_{\v{k}\,\up\dn;\,\v{k}'\,s_1's_2'}\psi(\v{k}')(i\sigma^y)_{s_1's_2'}\\
        &=\psi(g^{-1}:\v{k})\big(i\sigma^y)_{\up\dn} = \psi(g^{-1}:\v{k}).
    \end{split}
\end{equation}
In this calculation we inserted the expression of $D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}$ in Eq.~\eqref{eq:Group:Prod:BCSProdMatrix} and used
the equation $D_gi\sigma^y D_g^\trans = i\sigma^y$, where $D_g$ are the spin representation matrices given in Eq.~\eqref{eq:Group:Prod:spinRepMatrix}.

To find the transformation properties of $\v{d}_\v{k}$ the principle is the same as above for $\psi(\v{k})$ but the calculations become more involved.
We assume that the spin-momentum basis expansion of state $\ket{v}$ consists of only odd coefficients so that
$v_{s_1s_2}(\v{k}) = \v{d}_\v{k}\cdot(\v{\sigma}i\sigma^y)_{s_1s_2}$. Inserting this into the active transformation of the coefficients of $\ket{V}$ in
Eq.~\eqref{eq:Group:pdRep:coeffTrans} and also inserting the expression for $D^{(D\times D)}_{\v{k}\,s_1s_2;\,\v{k}'\,s_1's_2'}$ as we did before yields
\begin{equation}
    \label{eq:Group:pdRep:oddCoeffTrans}
    \begin{split}
        g:v_{s_1s_2}(\v{k}) &= \sum_{\v{k}'\,s_1's_2'}\delta_{\v{k},g:\v{k}'}D_{g\,s_1s_1'}D_{g\,s_2s_2}\v{d}_{\v{k}'}\cdot(\v{\sigma}i\sigma^y)_{s_1's_2'}\\
        &= \sum_s\Big(D_g\v{\sigma}\sigma^yD_g^\trans\sigma^y\Big)_{s_1s}\cdot\v{d}_{g^{-1}:\v{k}}\;i\sigma^y_{ss_2}.
    \end{split}
\end{equation}
Since a group transformation (aka. the linear transformation given by a group representation) can not bring a state that was even to be odd or vice versa,
then the resulting state given by the transformed coefficients $g:v_{s_1s_2}(\v{k})$ has to remain odd, and thus they can be expanded in terms of a new
$\v{d}'$ such that 
\begin{equation}
    \label{eq:Group:pdRep:oddCoeffTrans:transExpansion}
    g:v_{s_1s_2}(\v{k}) = \sum_s\v{d}'_{g^{-1}:\v{k}}\cdot\v{\sigma}_{s_1s}i\sigma^y_{ss_2}.
\end{equation}
Having expanded both sides of the transformed coefficients with a common factor $i\sigma^y$ to the right we can equate the remaining $2\times2$ spin
matrices which gives an expression for $\v{d}'_{g^{-1}:\v{k}}\cdot\v{\sigma}$ by comparing Eq.~\eqref{eq:Group:pdRep:oddCoeffTrans} and
Eq.~\eqref{eq:Group:pdRep:oddCoeffTrans:transExpansion}.
Furthermore, using the anti-commutation property $\{\sigma^i,\sigma^j\} = 2\delta_{ij}\sigma^0$ of Pauli matrices we find that
\begin{equation}
    \label{eq:Group:pdRep:projectingD}
    d'_{\v{k},i} = \frac{1}{4}\tr\big(\{\sigma^i, \v{d}'_\v{k}\cdot\v{\sigma}\big\}).
\end{equation}
Inserting the expression for $\v{d}'_{g^{-1}:\v{k}}\cdot\v{\sigma}$ and the full expression of the $SU(2)$ spin-representation matrices $D_g$, which
is found in  Eq.~\eqref{eq:Group:Prod:spinRepMatrix}, yields after some algebra
\begin{equation}
    \label{eq:Group:pdRep:transformedD}
    \begin{split}
        d'_{g^{-1}:\v{k},i} &= \frac{1}{4}\tr\big(\{\sigma^i, \v{d}_{g^{-1}:\v{k}}\cdot D_g\v{\sigma}\sigma^yD_v^\trans\sigma^y\}\big)\\
        &= R_{ij}(\hat{\v{u}},\phi)d_{g^{-1}:\v{k},j},
    \end{split}
\end{equation}
where we have defined the matrix
\begin{equation}
    \label{eq:Group:pdRep:rotMatrixIndices}
    \begin{split}
        &R_{ij}(\hat{\v{u}},\phi) = \delta_{ij}\cos\phi + \hat{u}_i\hat{u}_j(1-\cos\phi)-\epsilon_{ijk}\hat{u}_k\sin\phi\\
        & = 
        \left(\begin{smallmatrix}
            \cos\phi + \hat{u}_x^2(1-\cos\phi) & \hat{u}_x\hat{u}_y(1-\cos\phi)-\hat{u}_z\sin\phi & \hat{u}_x\hat{u}_z(1-\cos\phi)+\hat{u}_y\sin\phi\\
            \hat{u}_y\hat{u}_x(1-\cos\phi)+\hat{u}_z\sin\phi & \cos\phi+\hat{u}_y^2(1-\cos\phi) & \hat{u}_y\hat{u}_z(1-\cos\phi)-\hat{u}_x\sin\phi\\
            \hat{u}_z\hat{u}_x(1-\cos\phi)-\hat{u}_y\sin\phi & \hat{u}_z\hat{u}_y(1-\cos\phi)+\hat{u}_x\sin\phi & \cos\phi+\hat{u}_z^2(1-\cos\phi)
        \end{smallmatrix}\right).
    \end{split}
\end{equation}
This matrix is in fact the rotation matrix of a vector in $\mathbb{R}^3$ by an angle $\phi$ about a unit vector $\hat{\v{u}}$. Since the coefficients
 of an odd state $\ket{v}$ are fully determined by the vector $\v{d}$, their transformation can be regarded just as a tranformation of $\v{d}$ itself
 which thus takes the form
\begin{equation}
    \label{eq:Group:pdRep:dTranformation}
    g:d_{\v{k},i} = R_{ij}(\hat{\v{u}},\phi)d_{g^{-1}:\v{k},j},
\end{equation}
where $\hat{\v{u}}$ and $\phi$ gives the unit vector and angle respectively, of the proper rotation associated with $g$. The conclusion is thus that
$\v{d}$ transforms as a vector by the proper rotation associated with $g$.

\subsection{Projection Operators}
\label{sec:Group:Pro}

Let us assume that we are in a vector space $V$ that can be divided into possibly several different irreducible representations $D^{(\alpha)}$ of some
symmetry group $G$. Further, let the basis vectors of these irreducible representations be denoted by $\v{b}^{(\alpha)}_m$ where $m$ thus counts the number
of basis vectors in each \irr Then an arbitrary vector $\v{f}\in V$ can be written in terms of these basis vectors as
\begin{equation}
    \label{eq:Group:Irr:Pro:basisDecomp}
    \v{f} = \sum_\alpha\sum_m c_m^{(\alpha)}\v{b}^{(\alpha)}_m.
\end{equation}

A projection operator can be used to extract any combination of constant $c_m^{(\alpha)}$ multiplied by a basis vector $\v{b}^{(\alpha)}_n$, where
$m$ and $n$ can in general be different. Denoting the projection operator that picks out the $m$th constant multiplied by the $l$th basis vector
in the \irr $\beta$ of the expansion of $\v{f}$: $P^{(\beta)}_{lm}$, then
\begin{equation}
    \label{eq:Group:Irr:Pro:generalProApplication}
    P^{(\beta)}_{lm}\v{f} = c_m^{(\beta)}\v{b}_l^{(\beta)}.
\end{equation}
This is extremely useful in finding a bases for the irreducible representations.
To acheive this, the projection operator is defined as
\begin{equation}
    \label{eq:Group:Irr:Pro:proOpDef}
    P^{(\beta)}_{l,m} = \frac{d_\beta}{\abs{G}}\sum_{g\in G}D_{lm}^{(\beta)}(g)^\ast g:,
\end{equation}
where $d_\beta$ is the dimension of \irr $\beta$, $D_{lm}^{(\beta)}(g)$ is the $lm$ element of the matrix representation of the group element $g$ and
finally we have used the notation $g:$ to denote application on vectors by the relevant representation. An example is the application of $g$ to the
basis vectors $\v{b}_m^{(\alpha)}$. Since the relevant representation of $g$ in this case is the irreducible representation for which $\v{b}_m^{(\alpha)}$ is
a basis vector, the application becomes
\begin{equation}
    \label{eq:Group:Irr:Pro:gApplication}
    g : b_m^{(\alpha)} = \sum_nb_n^{(\alpha)}D_{nm}^{(\alpha)}(g).
\end{equation}

Usually, the full generality of the
projection operators $P^{(\beta)}_{l,m}$ isn't needed and it suffices to consider the diagonal projection operators
$P^{(\beta)}_{l,l} \equiv P^{(\beta)}_l$ or indeed their sum, in which case the resulting operator can be written only in terms of the \irr characters
$\chi^{(\alpha)}(g)$ since
\begin{equation}
    \label{eq:Group:Irr:Pro:charPro}
    P^{(\beta)}\equiv\sum_lP^{(\beta)}_l = \frac{d_\beta}{\abs{G}}\sum_{g\in G}\sum_lD_{ll}^{(\beta)}(g)^\ast g: = \frac{d_\beta}{\abs{G}}\sum_{g\in G}\chi^{(\beta)}(g)^\ast g:.
\end{equation}


\section{Fermionic symmetry transformations}

\section{Time-reversal symmetry}

\section{Symmetries of the Square Lattice}

The symmetry group of the square lattice is denoted $C_{4v}$ in the Sch\"onflies notation. It contains $8$ elements in total:
\begin{itemize}
    \item[$e$:] The identity element (do nothing),
    \item[$C_4$:] Rotation by $90^\circ$ in the positive direction (ccw),
    \item[$C_4^{-1}$:] Rotation by $90^\circ$ in the negative direction (cw),
    \item[$C_4^2$:] Rotation by $180^\circ$,
    \item[$\sigma_x$:] Mirror about the $zy$-plane,
    \item[$\sigma_y$:] Mirror about the $zx$-plane,
    \item[$\sigma_{d_1}$:] Mirror about the downwards diagonal plane\footnote{We are assuming the western bias of left-to-right movement
        here. More precisely it is the plane containing the $z$ axis and the axis $y=-x$},
    \item[$\sigma_{d_2}$:] Mirror about the upwards diagonal plane.
\end{itemize}
This results in the group multiplication table in Table~\ref{tab:Group:Symm:multTab}. We can check that this is correct by
performing the group transformations in the top row followed by the one in the left column and seeing that this results
in the group transformations where these two intersect. As an example consider the vector $(x,y)^\trans$. Transforming this
by the $90^\circ$ counter clockwise rotation $C_4$ we get $(-y,x)^\trans$. Then mirroring this result about the $yz$-plane
yields $(y,x)^\trans$. We now realize that this is the same as mirroring the original vector about the axis $y=x$, hence
$\sigma_xC_4=\sigma_{d_2}$ as the multiplication table says.
\begin{table}
    \centering
    \begin{tabular}{c|cccc|cccc}
        & $e$ & $C_4$ &$C_4^2$ & $C_4^{-1}$ & $\sigma_x$ & $\sigma_y$ & $\sigma_{d_1}$ & $\sigma_{d_2}$\\ \hline
        $e$ & $e$ & $C_4$ & $C_4^2$ & $C_4^{-1}$ & $\sigma_x$ & $\sigma_y$ & $\sigma_{d_1}$ & $\sigma_{d_2}$\\
        $C_4$ & $C_4$ & $C_4^2$ & $C_4^{-1}$ & $e$ & $\sigma_{d_1}$ & $\sigma_{d_2}$ & $\sigma_y$ & $\sigma_x$\\
        $C_4^2$ & $C_4^2$ & $C_4^{-1}$ & $e$ & $C_4$ & $\sigma_y$ & $\sigma_x$ & $\sigma_{d_2}$ & $\sigma_{d_1}$\\
        $C_4^{-1}$ & $C_4^{-1}$ & $e$ & $C_4$ & $C_4^2$ & $\sigma_{d_2}$ & $\sigma_{d_1}$ & $\sigma_x$ & $\sigma_y$\\ \hline
        $\sigma_x$ & $\sigma_x$ & $\sigma_{d_2}$ & $\sigma_y$ & $\sigma_{d_1}$ & $e$ & $C_4^2$ & $C_4^{-1}$ & $C_4$\\
        $\sigma_y$ & $\sigma_y$ & $\sigma_{d_1}$ & $\sigma_x$ & $\sigma_{d_2}$ & $C_4^2$ & $e$ & $C_4$ & $C_4^{-1}$\\
        $\sigma_{d_1}$ & $\sigma_{d_1}$ & $\sigma_x$ & $\sigma_{d_2}$ & $\sigma_y$ & $C_4$ & $C_4^{-1}$ & $e$ & $C_4^2$\\
        $\sigma_{d_2}$ & $\sigma_{d_2}$ & $\sigma_y$ & $\sigma_{d_1}$ & $\sigma_x$ & $C_4^{-1}$ & $C_4$ & $C_4^2$ & $e$
    \end{tabular}
    \caption{Group multiplication table of the group $C_{4v}$.}
    \label{tab:Group:Symm:multTab}
\end{table}

The conjugation classes of a group is the sets of group elements that is conjugate to each other, meaning that there exists a group element $g$ such
that $gAg^{-1} = B$ between conjugate elements $A$ and $B$. Since conjugation is an equivalence relation it subdivides the group elements
exactly into conjugation classes. The conjugation classes of the group $C_{4v}$ are $e = \{e\}$, $2C_4 = \{C_4,C_4^{-1}\}$, $C_4^2 = \{C_4^2\}$,
$2\sigma_d = \{\sigma_{d_1},\sigma_{d_2}\}$ and $2\sigma_v = \{\sigma_x,\sigma_y\}$. The character $\chi^\Gamma(g)$ of a representation $\Gamma$ is the trace of the representation
matrix $D^\Gamma(g)$ of a certain group element $g$. Since the trace is cyclic, then for conjugate group elements $A$ and $B$
\begin{equation}
    \label{eq:Group:Symm:conjugateCharacter}
    \begin{split}
        \chi^\Gamma(B) &= \tr\Big(D^\Gamma(g)D^\Gamma(A)D^\Gamma(g^{-1})\Big)\\
        &= \tr\Big(D^\Gamma(g^{-1})D^\Gamma(g)D^\Gamma(A)\Big)\\
        &= \tr\Big(D^\Gamma(g^{-1}g)D^\Gamma(A)\Big) = \chi^\Gamma(A).
    \end{split}
\end{equation}
This means that the representation of all group elements in a certain conjugation class has the same character. 

It is useful to list the characters of the different conjugation classes in a table of the different irreducible representations of a group.
This is because the number of conjugation classes of a finite group is the same as the number of irreducible representations of that group.
This table is known as the character table of the group. The character table of
the group $C_{4v}$ is shown in Table~\ref{tab:Group:Symm:characterTable}. This table can be derived without knowing the details of the irreducible
representations but in stead using character relations from basic group theory\footnote{Many of these relations are derived from the great orthogonality
theorem which can be found \eg in \cite{Inui90}.}. 

The dimensionality of the \irr can be found in Table~\ref{tab:Group:Symm:characterTable} by looking up the first column, i.e. the column giving
the character of the conjugation class $\{e\}$.
Since the group element $e$ maps to the identity transformation in all representations then its
trace gives the dimension of the representation. From the table we see that all the irreducible representations are $1$-dimensional except for
$\Gamma_5$ which is $2$ dimensional.
\begin{table}
    \centering
    \begin{tabular}{c|ccccc}
        $C_{4v}$ & $e$ & $2C_4$ & $C_4^2$ & $2\sigma_v$ & $2\sigma_d$\\ \hline
        $\Gamma_1$ & $1$ & $1$ & $1$ & $1$ & $1$\\
        $\Gamma_2$ & $1$ & $1$ & $1$ & $-1$ & $-1$\\
        $\Gamma_3$ & $1$ & $-1$ & $1$ & $1$ & $-1$\\
        $\Gamma_4$ & $1$ & $-1$ & $1$ & $-1$ & $1$\\
        $\Gamma_5$ & $2$ & $0$ & $-2$ & $0$ & $0$
    \end{tabular}
    \caption{Character table of the group $C_{4v}$.}
    \label{tab:Group:Symm:characterTable}
\end{table}
All the $1$-dimensional irreducible representation matrices are then completely determined by the character table since they are just given by
the characters themselves, \ie $D^{\Gamma_2}(\sigma_x) = -1$.

To find a $2$-dimensional representation of $C_{4v}$ we can imagine how a normal $2$D vector $(x,y)^\trans\in\mathbb{R}^2$ behaves under its transformations.
We take again the example of a counter clockwise rotation by $90^\circ$ which tranforms a vector
\begin{equation}
    \label{eq:Group:Symm:C4transform}
    C_4:\begin{pmatrix}
        x\\
        y
    \end{pmatrix} = 
    \begin{pmatrix}
        -y\\
        x
    \end{pmatrix} = 
    \begin{pmatrix}
        0 & -1\\
        1 & 0
    \end{pmatrix}
    \begin{pmatrix}
        x\\
        y
    \end{pmatrix}.
\end{equation}
Obviously, the matrix 
\begin{equation}
    \label{eq:Group:Symm:C4G5D}
    D^{(\Gamma_5)}(C_4) = 
    \begin{pmatrix}
        0 & -1\\
        1 & 0
    \end{pmatrix},
\end{equation}
is the representation matrix of a two-dimensional representation of $C_4$. Continuing in this way for all the group tranformations yields the matrices
\begin{subequations}
    \label{eq:Group:Symm:G5D}
\begin{align}
    D^{(\Gamma_5)}(e) &=
    \begin{pmatrix}
        1 & 0\\
        0 & 1
    \end{pmatrix}, & D^{(\Gamma_5)}(C_4) &=
    \begin{pmatrix}
        0 & -1\\
        1 & 0
    \end{pmatrix},\\
    D^{(\Gamma_5)}(C_4^{-1}) &=
    \begin{pmatrix}
        0 & 1\\
        -1 & 0
    \end{pmatrix}, & D^{(\Gamma_5)}(C_4^2) &= 
    \begin{pmatrix}
        -1 & 0\\
        0 & -1
    \end{pmatrix},\\
    D^{(\Gamma_5)}(\sigma_x) &= 
    \begin{pmatrix}
        -1 & 0\\
        0 & 1
    \end{pmatrix}, & D^{(\Gamma_5)}(\sigma_y) &=
    \begin{pmatrix}
        1 & 0\\
        0 & -1
    \end{pmatrix},\\
    D^{(\Gamma_5)}(\sigma_{d_1}) &=
    \begin{pmatrix}
        0 & -1\\
        -1 & 0
    \end{pmatrix}, & D^{(\Gamma_5)}(\sigma_{d_2}) &=
    \begin{pmatrix}
        0 & 1\\
        1 & 0
    \end{pmatrix}.
\end{align}
\end{subequations}


\section{Square Lattice Harmonics}

Since $\psi(\v{k})$ and $\v{d}_\v{k}$ are invariant with respect to translation by any resiprocal lattice vector $\v{Q}$: $\psi(\v{k}+\v{Q}) = \psi(\v{k})$,
they can be expanded in a discrete Fourier transform over the real lattice, such that
\begin{equation}
    \label{eq:Group:SLH:psiFourier}
    \psi(\v{k}) = \frac{1}{\sqrt{N}}\sum_\v{R}\psi_\v{R}\cos\v{R}\cdot\v{k},
\end{equation}
and
\begin{equation}
    \label{eq:Group:SLH:dFourier}
    \v{d}_\v{k} = \frac{1}{\sqrt{N}}\sum_\v{R}\v{d}_\v{R}\sin\v{R}\cdot\v{k},
\end{equation}
where the exponential of the Fourier transform has been reduced to trigonometric functions by the parity of the functions.

We are interested in the basis vectors $\ket{\Gamma,q,m}$ of the representations of the symmetry group $C_{4v}$ of the $2D$ square lattice. In this
ket notation, $\Gamma$ gives the \irr, $m$ enumerates the basis vectors in case the \irr is multi-dimensional, while $q$ gives the version of
the \irr in case the space of possible $\ket{v}$ permits multiple versions of the same \irr In the active
view of group transformations, the question of finding the basis vectors translates to finding the basis functions of the functions $\psi(\v{k})$
and $\v{d}_\v{k}$. In Section~\ref{sec:Group:pdRep} we
saw how these functions transformed under group transformations. In Section~\ref{sec:Group:Pro} we saw how the projection operators could be used
to extract individual basis vectors. We will now use these operators on the Fourier expansions of $\psi(\v{k})$ and $\v{d}_\v{k}$ to extract possible
basis functions given the symmetry group of the square lattice.

\subsection{Even basis functions}

\section{Decomposition of the Potential}

Let at first $\hat{V}$ be a general two-body operator that acts on an $N$-particle state which is a vector in $\hil_N = \otimes_{i=1}^N\hil$. The
single particle Hilbert space $\hil$ in question is quantified by momentum and spin such that $\hil=\Span\{\ket{\v{k},s}\}$. Denoting specific combinations
of $\v{k}$ and $s$ as $\alpha$ as a shorthand for the moment, then $\hat{V}$ acts on basis vectors in $\hil_N$ as
\begin{equation}
    \label{eq:Group:Potential:twoBodyInteraction}
    \hat{V}\ket{\alpha_1}\ldots\ket{\alpha_N} = \sum_{1\leq i<j\leq N}\hat{V}_{ij}\ket{\alpha_1}\ldots\ket{\alpha_N},
\end{equation}
by definition of being a two-body operator. Here $\hat{V}_{ij}$ is an operator that only acts on the $i$th and $j$th ket. Even though $\hat{V}$ acts on
$\hil_N$, because of how it can be written in terms of $\hat{V}_{ij}$ and this only acts on two states at a time, it follows that $\hat{V}$ is
completely determined by its action on the reduced Hilbert space $\hil_2$. This implies that $\hat{V}$ is fully described by its matrix elements
\begin{equation}
    \label{eq:Group:Potential:genTwoBodyMatrixElem}
    \bra{\alpha}\bra{\alpha'}\hat{V}\ket{\beta}\ket{\beta'}.
\end{equation}
Inserting back the $\ket{\v{k},s}$ notation, these matrix elements are referred to as
\begin{equation}
    \label{eq:Group:Potential:momSpinPotentialMatrixElems}
    V_{\v{k}_1\v{k}_2\v{k}_3\v{k}_4;\,s_1s_2s_3s_4} = \bra{\v{k}_1s_1}\bra{\v{k}_2s_2}\hat{V}\ket{\v{k}_4s_4}\ket{\v{k}_3s_3}.
\end{equation}
When $\hat{V}$ is a BCS operator acting on the BCS Hilbert space described in the previous section, these matrix elements are denoted
\begin{equation}
    \label{eq:Group:Potential:bcsTwoBodyMatrixElem}
    V_{\v{k}\v{k}';\,s_1s_2s_3s_4} = \bra{\v{k}s_1}\bra{-\v{k}s_2}\hat{V}\ket{\v{k}'s_4}\ket{-\v{k}'s_3}.
\end{equation}

Since $\hat{V}$ is Hermitian, it must be diagonalizable in a basis of eigenfunctions. Barring accidental degeneracy, a basis for a $d$-degenerate
eigenvalue is also a basis for an irreducible representation of the symmetry group $G$ of the Hamiltonian. In the case of accidental degeneracy
then this $d$-dimensional vector space consists of several non-intersecting subspaces where each subspace is a basis for a (possibly different) \irr Note
that this does not mean that (barring accidental degeneracy) there exists one seperate eigenvalue for each \irr of $G$ since there might be
several different eigenvalues with different eigenspace bases but where all of them are bases for the same \irr Regardless of these details,
the connection between irreducible representations and the eigenvalues of $\hat{V}$ is a great help in finding the bases for which it is
diagonal.

We let the basis for a $d_\Gamma$-dimensional \irr $\Gamma$ be denoted $\{\ket{\Gamma,q_\Gamma,m}\}_{m=1}^{d_\Gamma}$, where $\hat{V}$ has an eigenvalue $V_{\Gamma,q_\Gamma}$
for the vectors in this basis and $q_\Gamma$ is an index enumerating the different versions of bases of $\Gamma$ that $\hat{V}$ might have
in its set of eigenspace bases. Since $\hat{V}$ then is diagonal in this set of bases then
\begin{equation}
    \label{eq:Group:Potential:potIrrepDecomp}
    \hat{V} = \sum_{\Gamma q_\Gamma}V_{\Gamma,q_\Gamma}\sum_{m=1}^{d_\Gamma}\ket{\Gamma,q_\Gamma,m}\bra{\Gamma,q_\Gamma,m}.
\end{equation}
Because of the potential for accidental degeneracy we can not guarantee that $V_{\Gamma,q_\Gamma}\neq V_{\Gamma',q_{\Gamma'}}$ for different $\Gamma$ and $\Gamma'$.
Inserting this expression for $\hat{V}$ into the matrix elements in Eq.~\eqref{eq:Group:Potential:bcsTwoBodyMatrixElem} lets us write them in terms of
irreducible representation basis vectors in the momentum spin function representation:
\begin{equation}
    \label{eq:Group:Potential:irrepPotMatrixElem}
    V_{\v{k}\v{k}';\,s_1s_2s_3s_4} = \sum_\Gamma V_{\Gamma,q_\Gamma}\sum_{m=1}^{d_\Gamma}\Psi_{s_1s_2}^{\Gamma,q_\Gamma}(\v{k})\Psi_{s_3s_4}^{\Gamma,q_\Gamma}(-\v{k}')^\dagger,
\end{equation}
where
\begin{equation}
    \label{eq:Group:Potential:irrepPotMatrixElem:coeff}
    \Psi_{s_1s_2}^{\Gamma,q_\Gamma}(\v{k}) = \bra{\v{k},s_1}\braket{-\v{k},s_2}{\Gamma,q_\Gamma,m}.
\end{equation}

We can separate the set of different irreducible representation bases into bases that have vectors that transform either symmetrically or anti-symmetrically
with respect to the group element of space inversion $P$. We call the representations of such bases even or odd representations. Even representations are those that map
$P$ to the identity operator $\mathbb{1}$ and as a consequence have $\Psi^{\Gamma,q_\Gamma,m}_{s_1s_2}(-\v{k}) = \Psi^{\Gamma,q_\Gamma,m}_{s_1s_2}(\v{k})$. Writing
the spin-indices in these functions in terms of Pauli matrices by using the expansion in Eq.~\eqref{eq:Group:BCSHil:arb:pauli} and using the fermionic symmetry then
even representations $a$ have
\begin{equation}
    \label{eq:Group:Potential:evenIrrepFunctions}
    \Psi^{a,q_a,m}_{s_1s_2}(\v{k}) = \psi^{a,q_a,m}_\v{k}i\sigma^y_{s_1s_2}.
\end{equation}
Odd representations $b$ map $P$ to the inversion operator $I$ such that $\Psi^{b,q_b,m}_{s_1s_2}(-\v{k}) = -\Psi^{b,q_b,m}_{s_1s_2}(\v{k})$. Expanding in Pauli
matrices then yields
\begin{equation}
    \label{eq:Group:Potential:oddIrrepFunctions}
    \Psi^{b,q_b,m}_{s_1s_2}(\v{k}) = \v{d}^{b,q_b,m}_{\v{k}}\cdot(\v{\sigma}i\sigma^y)_{s_1s_2}.
\end{equation}
Separating the sum over irreducible representations $\Gamma$ into sums over even ($a$) and odd ($b$) representations in the potential operator matrix elements in
Eq.~\eqref{eq:Group:Potential:irrepPotMatrixElem}, we arrive at the fully expanded expression
\begin{equation}
    \label{eq:Group:Potential:fullyExpandedPotMatrixElem}
    \begin{split}
        &V_{\v{k}\v{k}';\,s_1s_2s_3s_4} = \sum_{aq_a}V_{a,q_a}\sum_{m=1}^{d_a}\psi_\v{k}^{a,q_a,m}i\sigma^y_{s_1s_2}\big(\psi_{-\v{k}'}^{a,q_a,m}i\sigma^y_{s_3s_4}\big)^\dagger\\
        & + \sum_{bq_b}V_{b,q_b}\sum_{m=1}^{d_b}\big(\v{d}^{b,q_b,m}_{\v{k}}\cdot\v{\sigma}i\sigma^y\big)_{s_1s_2}\Big[\big(\v{d}^{b,q_b,m}_{-\v{k}'}\cdot\v{\sigma}i\sigma^y\big)_{s_3s_4}\Big]^\dagger.
    \end{split}
\end{equation}
In this use of the dagger notation, the adjoint acts on both of the matrix indices such that $\v{d}_{-\v{k}}^\dagger = \v{d}_\v{k}^\ast$ and $\sigma_{s_1s_2}^\dagger = \sigma_{s_2s_1}^\ast$.
