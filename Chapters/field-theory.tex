\chapter{Field Theory Methods}
\label{chap:Field}
%
\noindent In this chapter we will give a short introduction to the use of Gra\ss mann variables
and complex numbers in the calculation of the field-integrals in the
partition function and how they can be used to transform the
expression for the action through the \ac{hs} transformation.

A field theoretic expression for the quantum mechanical partition function $\mathcal{Z}$
is obtained by using a coherent state basis. A coherent state is the eigen-state of an annihilation operator, thus it
produces an eigenvalue when operated on by the annihilation operator. Letting $\hat{H}$ be the quantum mechanical
Hamiltonian of the system for which we are interested in calculating the partition function, $\mu$ be the chemical
potential and $\hat{N}$ be the number operator, then the partition function is defined as
\begin{equation}
    \label{eq:Field:partitionFunction}
    \mathcal{Z} = \tr(e^{-\beta(\hat{H}-\mu\hat{N})}).
\end{equation}
Inserting a basis of coherent states $\{\ket{\xi}\}$ when calculating the trace, we obtain a functional integral over
the coherent state eigenvalues $\xi_\alpha$ and $\xi^\ast_\alpha$ by substituting these variables for
$c_\alpha$ and $c_\alpha^\dagger$ respectively in the $\hat{H}$- and $\hat{N}$-operators. Here $\alpha$ symbolizes the set of quantum-numbers needed to specify
a state. The functional integral then takes the form
\begin{equation}
    \label{eq:Field:fieldPartition}
    \mathcal{Z} = \int\!\mathcal{D}[\xi^\ast\,\xi]e^{-\int_0^\beta\!\mathrm{d}\tau\sum_\alpha [\xi^\ast_\alpha(\partial_\tau-\mu)\xi_\alpha + H(\xi_\alpha,\xi_\alpha^\ast)]}.
\end{equation}
The integration variable $\tau$ is the imaginary time and a $\tau$-dependence is implicit in the notation such that $\xi_\alpha = \xi_\alpha(\tau)$.
This path-integral notation is a shorthand for a more involved expression where the imaginary time-dependence of $\tau$ is
split into a collection of time-indexed coherent state eigenfunctions $\xi_{\alpha,\tau}$ and the integration measure
is a product over these indices and the quantum-state indices $\alpha$. For further detail we refer to Ref.~\cite{NegeleOrland98}
which we will follow for a large part of this chapter.

\section{Quadratic Fermionic Field Integrals}

Because of the anti-commuting property of the fermion annihilation operators, any coherent state has to have eigenvalues that anti-commute as well. 
This leads to Gra\ss mann numbers being the central variables used in constructing the partition function when it 
is written in the convenient basis of coherent states.

\subsection{Gra\ss mann algebras}
A Gra\ss mann algebra is constructed on a set of generators $\{\xi_\alpha\}$ such that a specific product
of the generators $\xi_{\alpha_1}\xi_{\alpha_2}\cdots\xi_{\alpha_n}$ together with a complex coefficient $\phi$ constitute a
number in the algebra and the generators anti-commute such that $\xi_\alpha\xi_\beta = -\xi_\beta\xi_\alpha$. On such
an algebra, differentiation can be defined such that
\begin{equation}
    \label{eq:Field:Ferm:Grass:diff}
    \frac{\mathrm{d}}{\mathrm{d}\xi_{\alpha_m}}\;\phi\,\xi_{\alpha_1}\cdots\xi_{\alpha_n} = (-1)^{m-1}\phi\,\xi_{\alpha_1}\cdots\xi_{\alpha_{m-1}}\xi_{\alpha_{m+1}}\cdots\xi_{\alpha_n},
\end{equation}
provided the generator $\xi_{\alpha_m}$ is in the number and $0$ otherwise. The factors of $(-1)$ comes from anti-commuting
the generator $\xi_{\alpha_m}$ such that it is next to the differentiation operator. In Gra\ss mann algebra, integration
can (perhaps a little non-intuitively) be defined such that it acts in the same way as differentiation, i.e. generators
have to be anti-commuted until they are next to the symbolic infinitesimal differential $\mathrm{d}\xi_{\alpha}$, and then use
\begin{equation}
    \label{eq:Field:Ferm:Grass:int}
    \int\!\mathrm{d}\xi\;\xi = 1,
\end{equation}
while
\begin{equation}
    \label{eq:Field:Ferm:Grass:int2}
    \int\!\mathrm{d}\xi\;1 = 0.
\end{equation}
If the integral consists of several differentials of generators, then these differentials also has to be anti-commuted such that $\mathrm{d}\xi_1\mathrm{d}\xi_2 = -\mathrm{d}\xi_2\mathrm{d}\xi_1$.
On an algebra consisting of $2n$ generators we define conjugation as a map from the first half of the generators $\{\xi_{\alpha_i}\}_{i=1}^n$
to the other half $\{\xi_{\alpha_i}^\ast\}_{i=1}^n$ and in such a way that when applied to a particular number
\begin{equation}
    \label{eq:Field:Ferm:Grass:conj}
    (\phi\xi_\alpha\xi_\beta)^\ast = \phi^\ast\xi_\beta^\ast\xi_\alpha^\ast,
\end{equation}
for $\phi\in\mathbb{C}$.

\subsection{Nambu Spinor}

In the Nambu notation we group spin-dependent Gra\ss mann numbers $\xi_\up$ and $\xi_\dn^\ast$, which correspond to the
annihilation- and creation-operators $\hat{c}_\up^\dagger$ and $\hat{c}_\dn$, in a vector called a Nambu spinor
\begin{equation}
    \label{eq:Field:Ferm:Nambu:vector}
    \v{\xi} = 
    \begin{pmatrix}
        \xi_\up\\
        \xi_\dn^\ast
    \end{pmatrix}.
\end{equation}
A sesquilinear form can then be created with this vector and its adjoint such that
\begin{equation}
    \label{eq:Field:Ferm:Nambu:terms}
    \v{\xi}^\dagger S\v{\xi} = S_{11}\xi_\up^\ast\xi_\up + S_{22}\xi_\dn^\ast\xi_\dn + S_{12}\xi_\up^\ast\xi_\dn^\ast + S_{21}\xi_\up\xi_\dn.
\end{equation}
This allows any action that contains spin-dependent terms of the form of the right hand side of Eq.~\eqref{eq:Field:Ferm:Nambu:terms} to
be put on sesquilinear form. Assuming this is the case, then the partition function in the field-integral representation takes the form
\begin{equation}
    \label{eq:Field:Ferm:Nambu:partitionFunction}
    \mathcal{Z} = \int\!\mathcal{D}[\xi^\ast\,\xi]\;e^{-\int_0^\beta\!\mathrm{d}\tau\;\v{\xi}^\dagger_\gamma S_{\gamma\delta}\v{\xi}_\delta}.
\end{equation}
In this equation, the indices $\gamma$ and $\delta$ is an arbitrary collection of quantum numbers needed to specify a state other than spin,
for example they could be momentum indices $\gamma = \v{k}, \delta = \v{k}'$, and summation over these repeated indices is implicitly understood.

Splitting the integral over $\tau$ into $M$ imaginary time-slices and expanding the path integral measure into a product of individual integrals
over specific quantum numbered and time-sliced Gra\ss mann variables such that 
\begin{equation}
    \label{eq:Field:Ferm:Nambu:fieldMeasure}
    \int\!\mathcal{D}[\xi^\ast\,\xi] \;\propto\; \lim_{M\rightarrow\infty}\int\prod_{\tau=1}^M\prod_\alpha\mathrm{d}\xi^\ast_{\alpha,\tau}\mathrm{d}\xi_{\alpha,\tau},
\end{equation}
the path-integral in Eq.~\eqref{eq:Field:Ferm:Nambu:partitionFunction}
can be evaluated by the Gaussian Gra\ss mann integral identity
\begin{equation}
    \label{eq:Field:Ferm:Nambu:quadraticGrassmannInt}
    \int\prod_i(\mathrm{d}\xi_i^\ast\mathrm{d}\xi_i)\;e^{-\xi_i^\ast S_{ij}\xi_j} = \det S,
\end{equation}
for which a derivation can be found in Ref.~\cite{NegeleOrland98}. This identity holds for any Hermitian matrix $S$, even if it is not positive definite.
The result is then that the partition function in Eq.~\eqref{eq:Field:Ferm:Nambu:partitionFunction}
becomes $\mathcal{Z} = \det S$. To calculate this determinant one has to consider the matrix $S$ as also a matrix with
time-slice indices. This is perhaps most easily accomplished using the Matsubara formalism in which the $\tau$ dependence is substituted with a dependence on Matsubara
frequencies through a Fourier-like transform. More details on this formalism can be found in Section~\ref{sec:Field:Mats}, but first we consider
what to do when a spin-dependent action can't be written on the form in Eq.~\eqref{eq:Field:Ferm:Nambu:terms}.

\subsection{Extended Nambu Spinor}

From Eq.~\eqref{eq:Field:Ferm:Nambu:terms} we see that the Nambu spinor sesquilinear product fails to accommodate terms in a Hamiltonian that mix creation and annihilation
operators of differing spins, e.g. a term $\propto \hat{c}_\up^\dagger\hat{c}_\dn$. 
In general, a quadratic Hamiltonian can contain any combination of spin-indices of the form $\hat{c}_{s_1}\hat{c}_{s_2}$, $\hat{c}_{s_1}^\dagger\hat{c}_{s_2}$,
$\hat{c}_{s_1}\hat{c}_{s_2}^\dagger$ and
$\hat{c}_{s_1}^\dagger\hat{c}_{s_2}^\dagger$. This gives in total $16$ different combinations and to accommodate them all we thus need a $4\times4$ matrix.
Exchanging to Gra\ss mann numbers we define the vector
\begin{equation}
    \label{eq:Field:Ferm:ExNambu:spinor}
    \v{\xi}_\gamma = 
    \begin{pmatrix}
        \xi_{\gamma,\up}^\ast\\
        \xi_{\gamma,\up}\\
        \xi_{\gamma,\dn}^\ast\\
        \xi_{\gamma,\dn}
    \end{pmatrix},
\end{equation}
where all quantum numbers except spin is included in the index $\gamma$.
Writing the elements of this vector $(\v{\xi}_\gamma)_i = \tilde{\xi}_{\gamma,i}$ regardless of whether it is a conjugate or not, 
we can write all quadratic terms of a Hamiltonian on the bilinear form
\begin{equation}
    \label{eq:Field:Ferm:ExNambu:bilinear}
    \v{\xi}_\gamma^\trans S_{\gamma\delta}\v{\xi}_\delta = \tilde{\xi}_{\gamma,i}S_{\gamma i; \delta j}\tilde{\xi}_{\delta, j},
\end{equation}
where $S_{\gamma\delta}$ is a $4\times4$ antisymmetric\footnote{To see why this matrix can always be said to be antisymmetric lets first simplify the notation
and write the bilinear product as $\tilde{\xi}_iS_{ij}\tilde{\xi}_j$. Then the matrix $S = (S + S^\trans)/2 + (S - S^\trans)/2$, such that we can write it as a
symmetric matrix ${\mathcal{S} = (S+S^\trans)/2}$ and an antisymmetric matrix $\mathcal{A} = (S-S^\trans)/2$. Considering only the symmetric part of the bilinear
form we get
\begin{equation*}
    \begin{split}
        \tilde{\xi}_i\mathcal{S}_{ij}\tilde{xi}_j = -\tilde{\xi}_j\mathcal{S}_{ij}\tilde{\xi}_i = -\tilde{\xi}_i\mathcal{S}_{ji}\tilde{\xi}_j = -\tilde{\xi}_i\mathcal{S}_{ij}\tilde{\xi}_j.
    \end{split}
\end{equation*}
Hence $\tilde{\xi}_i\mathcal{S}_{ij}\tilde{\xi}_j = 0$ and all that remains is the antisymmetric bilinear form.}
matrix, and $S_{\gamma i; \delta j}$ denotes its elements. Let there be $n$ number of different quantum numbers,
now including spin. Then there must be $2n$ different Gra\ss mann generators $\tilde{\xi}_{\gamma,i}$. All of these are integrated over in the discrete version
of the partition function field integral
\begin{equation}
    \label{eq:Field:Ferm:ExNambu:partitionFunction}
    \mathcal{Z} = \int\!\mathcal{D}[\xi^\ast\,\xi]\;e^{-\int_0^\beta\!\mathrm{d}\tau\;\tilde{\xi}_{\gamma,i}S_{\gamma i; \delta j}\tilde{\xi}_{\delta, j}}.
\end{equation}
Even though this superficially looks like the field integral in Eq.~\eqref{eq:Field:Ferm:Nambu:partitionFunction}, we now have a bilinear and not a sesquilinear
form, and $S$ is now a $2n\times2n$ matrix and not an $n\times n$ matrix. This means that we can not use the integral in Eq.~\eqref{eq:Field:Ferm:Nambu:quadraticGrassmannInt}
to evaluate the integral, but instead have to rely on the more general Gaussian Gra\ss mann integral
\begin{equation}
    \label{eq:Field:Ferm:ExNambu:pfaffianIntegral}
    \int\prod_{i}(\mathrm{d}\tilde{\xi}_i)\;e^{-\frac{1}{2}\tilde{\xi}_iS_{ij}\tilde{\xi}_j} = \pfaff(S),
\end{equation}
which applies for any antisymmetric matrix $S$. The right hand side is called the Pfaffian $\pfaff(S)$ of the matrix $S$ and is defined for any antisymmetric
matrix to be given by
\begin{equation}
    \label{eq:Field:Ferm:ExNambu:pfaffianDef}
    \pfaff[S] = \frac{1}{2^nn!}\sum_{P\in S_n}(-1)^PS_{P_1P_2}\cdots S_{P_{n-1}P_n},
\end{equation}
where $P$ is a permutation in the finite group $S_n$ of all possible permutations of $n$ numbers. This matrix function is related to the determinant
by the relation $\pfaff(S)^2 = \det(S)$. 

Applying the integral identity in Eq.~\eqref{eq:Field:Ferm:ExNambu:pfaffianIntegral} to the partition function%
%
\footnote{In relating the discrete version of
Eq.~\eqref{eq:Field:Ferm:ExNambu:pfaffianIntegral} to \eqref{eq:Field:Ferm:ExNambu:partitionFunction} we have to make sure that the spinor elements $\tilde{\xi}_i$
are defined in terms of $\xi_i$ and $\xi_i^\ast$ in such a way as to get a correspondence to the sequence of Gra\ss mann
generators $\mathrm{d}\xi^\ast_i\mathrm{d}\xi_i$ in the measure to avoid any sign errors. One solution
is to set $\xi_i^\ast = \tilde{\xi}_{2i-1}$ and $\xi_i = \tilde{\xi}_{2i}$ as we have done in Eq.~\eqref{eq:Field:Ferm:ExNambu:spinor}. With this definition,
then the measure $\int\prod_i\mathrm{d}\xi_i^\ast\mathrm{d}\xi_i$, which results from the discretized version of the field-integral measure, becomes
equal to $\int\prod_{i=1}^{2n}\mathrm{d}\tilde{\xi}_i$ such that Eq.~\eqref{eq:Field:Ferm:ExNambu:pfaffianIntegral} can be directly applied.%
} %
%
in Eq.~\eqref{eq:Field:Ferm:ExNambu:partitionFunction}
after applying the proper discretization of the imaginary time then yields the result
\begin{equation}
    \label{eq:Field:Ferm:ExNambu:partitionFinal}
    \mathcal{Z} = \int\!\mathcal{D}[\xi^\ast\,\xi]\;e^{-\int_0^\beta\!\mathrm{d}\tau\;\tilde{\xi}_{\gamma,i}S_{\gamma i; \delta j}\tilde{\xi}_{\delta, j}} = \sqrt{\det(S)}.
\end{equation}
We have chosen the positive result in $\pfaff(S) = \pm\sqrt{\det(S)}$ since the partition function $\mathcal{Z}$ needs to be positive on physical grounds.
The matrix $S$ on the right hand side of Eq.~\eqref{eq:Field:Ferm:ExNambu:partitionFinal}, which we take the determinant of, is the full matrix one gets after
discretizing the imaginary time into slices which is usually done through the Matsubara-frquency formalism.

Now that we know that the partition function is given in terms of the determinant of the action-matrix $S$, we can use this information to manipulate the definition
of $\tilde{\xi}_{\gamma,i}$ so that we can still write the action as a sesquilinear form. In particular, switching the position of $\xi_{\gamma,s}^\ast$ and
$\xi_{\gamma,s}$ for both spins in the transposed vector on the left of the bilinear form $\v{\xi}_\gamma^\trans S_{\gamma\delta} \v{\xi}$, the transposed
vector becomes the adjoint vector. This affects the matrix $S$ by switching two pairs of rows. Denoting the matrix where the rows are switched $S'$, 
we can thus rewrite the bilinear form such that
\begin{equation}
    \label{eq:Field:Ferm:ExNambu:bilinearToSesqui}
    \v{\xi}_\gamma^\trans S_{\gamma\delta}\v{\xi}_\delta = \v{\xi}_\gamma^\dagger S'_{\gamma\delta}\v{\xi}_\delta.
\end{equation}
Now the integral over the exponent has not changed since all we have done is simply re-ordering its terms. However, since exchange of rows in a determinant
at most produces a minus sign and we do this twice we get that $\det S = \det S'$ and we can write
\begin{equation}
    \label{eq:Field:Ferm:ExNambu:partitionFunction:sesquilinear}
    \mathcal{Z} = \int\!\mathcal{D}[\xi^\ast\,\xi]\;e^{-\int_0^\beta\!\mathrm{d}\tau\;\v{\xi}_{\gamma}^\dagger S'_{\gamma\delta}\v{\xi}_{\delta}} = \sqrt{\det(S')}.
\end{equation}
where we need to remember that $S'$ is the row switched matrix of an antisymmetric matrix $S$.


\section{Matsubara formalism}
\label{sec:Field:Mats}

The Matsubara formalism\footnote{Named after the Japanese physicist Matsubara, Takeo.} is a way of handling the imaginary time $\tau$
dependence of the coherent state eigenvalue fields $\xi_\alpha(\tau)$, where $\alpha$ denotes
a collection of quantum numbers that are necessary to specify a state, without having to go back to the time-sliced path-integral. It also lets us
automatically satisfy the imaginary-time boundary conditions $\xi_\alpha(0) = \zeta\xi_\alpha(\beta)$, where $\zeta = +1$ for bosons
and $\zeta = -1$ for fermions. Imagining that $\tau$ is a continuous
variable as suggested in the path-integral notation, we define two countable infinite sets of new field-variables through the Fourier-transforms
\begin{subequations}
    \label{eq:Field:Mats:transformDef}
    \begin{align}
    \xi_{\alpha,n} &= \frac{1}{\sqrt{\beta}}\int_0^\beta\!\!\mathrm{d}\tau\;e^{i\omega_n\tau}\xi_\alpha(\tau),\label{eq:Field:Mats:transformDef:xi}\\
    \xi_{\alpha,n}^\ast &= \frac{1}{\sqrt{\beta}}\int_0^\beta\!\!\mathrm{d}\tau\;e^{-i\omega_n\tau}\xi_\alpha^\ast(\tau).\label{eq:Field:Mats:transformDef:xiAst}
    \end{align}
\end{subequations}
The frequencies $\omega_n$ are called Matsubara frequencies and are defined by $\omega_n = (2n+1)\pi/\beta$ with $n\in\mathbb{Z}$ for fermions. For bosons we use instead the notation
$\nu_n$ where $\nu_n = 2n\pi/\beta$. The inverse relations are given by 
\begin{subequations}
    \label{eq:Field:Mats:inverseTransform}
    \begin{align}
        \xi_\alpha(\tau) &= \sum_{n\in\mathbb{Z}}e^{-i\omega_n\tau}\xi_{\alpha,n},\label{eq:Field:Mats:inverseTransform:xi}\\
        \xi_\alpha^\ast(\tau) &= \sum_{n\in\mathbb{Z}}e^{i\omega_n\tau}\xi_{\alpha,n}^\ast.\label{eq:Field:Mats:inverseTransform:xiAst}
    \end{align}
\end{subequations}

\subsection{Matsubara sums}

When the Matsubara formalism $\xi_{\alpha,n}$ is used for the field variables in the action of a partition-function field-integral we will often
need to evaluate infinite  sums of Matsubara frequencies of the form
\begin{equation}
    \label{eq:Field:Mats:genSum}
    \sum_nh(if_n),
\end{equation}
where $f_n$ is either a fermionic- or bosonic Matsubara-frequency, 
to evaluate the field integral. A useful strategy in such evaluations is to transform the sum to a complex integral by using reverse residue
integration. We consider the complex contour integral along a path $\mathcal{C}$ running counter-clockwise around the complex plane infinitesimally close to
the imaginary axis as shown in Fig.~\ref{fig:Field:Mats:integrationContour}. 
%
\newcommand\hor{1.5}
\begin{figure}[h]
  \centering
  \begin{tikzpicture}[scale=0.7]
	\def\cross[#1,#2,#3]{% Defines a diagonal cross with three arguments: x-pos, y-pos and size.
	  \draw [violet] (#1-#3/2,#2-#3/2) -- (#1+#3/2,#2+#3/2);
	  \draw [violet] (#1+#3/2,#2-#3/2) -- (#1-#3/2,#2+#3/2);
	}
    % Enlarged integration contour starting from upper left.
    \draw[green, thick, dashed, decoration={markings, mark=at position 0.5 with {\arrow{latex}}}, postaction={decorate}] (-2.4,3.5) to[out=220, in=90] (-3.8, 0.55);
    \draw[green, thick, dashed] (-3.8, 0.55) -- (-3.5+\hor, 0.55);
    % Start of left small circle
    \draw[green, thick, dashed, decoration={markings, mark=at position 1.0 with {\arrow{latex}}}, postaction={decorate}] (-3.5+\hor, 0.55) to[out=90, in=180] (-3.2+\hor, 0.8);
    \draw[green, thick, dashed] (-3.2+\hor, 0.8) to[out=0, in=90] (-2.9+\hor, 0.5);
    \draw[green, thick, dashed, decoration={markings, mark=at position 1.0 with {\arrow{latex}}}, postaction={decorate}] (-2.9+\hor, 0.5) to[out=270, in=0] (-3.2+\hor, 0.2);
    \draw[green, thick, dashed] (-3.2+\hor, 0.2) to[out=180, in=270] (-3.5+\hor, 0.45);
    % End circle
    \draw[green, thick, dashed] (-3.5+\hor, 0.45) -- (-3.8, 0.45);
    \draw[green, thick, dashed] (-3.8, 0.45) -- (-3.8, -0.55);
    \draw[green, thick, dashed, decoration={markings, mark=at position 0.5 with {\arrow{latex}}}, postaction={decorate}] (-3.8, -0.55) to[out=270, in=130] (-2.4,-4);
    % End left contour
    % Branch cut
    \foreach \x in {2.0, 2.2, ..., 3.8}
    \draw [violet] (\x-0.1,-0.05) -- (\x, 0.05) (\x, 0.05) -- (\x+0.1,-0.05);
    % End branch cut
    % Enlarged countour right
    \draw[green, thick, dashed, decoration={markings, mark=at position 0.5 with {\arrow{latex}}}, postaction={decorate}] (2.4,-4) to[out=50, in=270] (3.9, -0.1);
    \draw[green, thick, dashed, decoration={markings, mark=at position 0.5 with {\arrow{latex}}}, postaction={decorate}] (3.9, -0.1) -- (1.8, -0.1);
    \draw[green, thick, dashed] (1.8, -0.1) to[out=180, in=180] (1.8, 0.1);
    \draw[green, thick, dashed, decoration={markings, mark=at position 0.5 with {\arrow{latex}}}, postaction={decorate}] (1.8, 0.1) -- (3.9, 0.1);
    \draw[green, thick, dashed, decoration={markings, mark=at position 0.5 with {\arrow{latex}}}, postaction={decorate}] (3.9, 0.1) to[out=90, in=320] (2.4,3.5);

  % Axis
  \draw[->] (0,-4) -- (0,4) node[right] {$\Im$};
  \draw[->] (-4,0) -- (4.1,0);
  \node at (4.25,-0.3) {$\Re$};
  \node at (0.9, 1.0) {$\mathcal{C}$};
  \node at (3.5, 3.0) {$\mathcal{C}'$};
  % Complex plane node
  \node at (-3.5,3.5) {$\mathbb{C}$};
  % Imaginary axis poles
  \foreach \a in {-3.75, -3.25,...,3.75}
  	\cross[0,\a,0.2];
  % Original contour
  \draw[green, thick, decoration={markings,
  		mark=at position 0.2 with {\arrow{latex}},
	  mark=at position 0.9 with {\arrow{latex}}}, postaction={decorate}] (0.5,-4) .. controls(0.6,0) .. (0.5,3.5);
  \draw[green, thick, decoration={markings,
  		mark=at position 0.2 with {\arrow{latex}},
	  mark=at position 0.9 with {\arrow{latex}}}, postaction={decorate}] (-0.5,3.5) .. controls(-0.6,0) .. (-0.5,-4);
  % Left pole
  \cross[-3.2+\hor,0.5,0.2];
\end{tikzpicture}
\caption{Integration contour for the Matsubara sum $\sum_nh(if_n)$. The contour is marked by a solid line and is imagined to continue
to $\pm i\infty$.
The crosses along the imaginary axis symbolizes the simple poles of the Fermi-Dirac distribution-function.
A deformed integration contour is shown with dashed lines that is imagined to cross the real axis at $\pm\infty$.
This contour then encloses a simple pole on the left and a branch cut on the right belonging to the summand.}
  \label{fig:Field:Mats:integrationContour}
\end{figure}
%
The integrand we consider is given by the product of the summand
and the complex continuation of the Fermi-Dirac- or Bose-Einstein-distribution function
\begin{equation}
    \label{eq:Field:Mats:distributionFunction}
    n_\zeta(z) = (e^{\beta z} - \zeta)^{-1},\quad z\in\mathbb{C},
\end{equation}
depending on whether the Matsubara
frequency in the sum is of fermionic ($\zeta = +1$) or bosonic ($\zeta = -1$) nature. This function has simple poles\footnote{That
the poles are simple, i.e. $1$st order, is easily seen by expanding the exponential around $if_n$ to leading order.} %
 at $z = if_n$ and thus integration
around the contour results in a sum of residues of the integrand at these poles such that we get
\begin{equation}
    \label{eq:Field:Mats:sumToIntegral}
    \sum_nh(if_n) = \frac{\zeta\beta}{2\pi i}\oint_{\mathcal{C}}\!\mathrm{d}z\;h(z)n_\zeta(z),
\end{equation}
given that $h(z)$ does not contain any poles at these points. The contour can now be continuously deformed at will as long as it does not cross any
singularities which can greatly facilitate the calculation of the integral. The default approach is to see if the integrand vanishes as $\abs{z}\to\infty$,
in which case it is usually useful to expand the contour as much as possible as illustrated by the deformed contour $\mathcal{C}'$ in
Figure~\ref{fig:Field:Mats:integrationContour}. 

Using the method outlined above, we may calculate the sums
\begin{subequations}
    \label{eq:Field:Mats:sums}
    \begin{align}
        \label{eq:Field:mats:sums:singlePole}
        \sum_{n\in\mathbb{Z}}\frac{1}{if_n-x} &= -\zeta\beta n_\zeta(x),\\
        \sum_{n\in\mathbb{Z}}\frac{1}{(i\omega_n - x)(i\omega_n - y)} &= \frac{\beta}{x-y}\bigg(\frac{1}{1+e^{\beta x}} - \frac{1}{1+e^{\beta y}}\bigg),\label{eq:Field:Mats:sums:twoPoles}\\
        \sum_{n\in\mathbb{Z}}\ln[\beta(i\omega_n + x)] &= \ln(1+e^{-\beta x}).\label{eq:Field:Mats:sums:ln}
    \end{align}
\end{subequations}

\section{Hubbard-Stratonovich transformation}

The \ac{hs} transformation is a transformation in the fields of a theory where a new complex field is introduced
in order to convert a term that is quadratic in an existing field variable, into a linear coupling between
the existing- and new field. This is particularly useful when the existing field is Fermionic and thus
a Gra\ss mann variable since it makes it possible to consider low energy exitations of the theory using \eg a saddle-point
approximation. It is however important to point out that the transformation itself is not in any way approximative, but is
an exact transformation that maintains all information of the original theory.

In technical terms, the \ac{hs} transformation can be viewed simply as the solution of a complex multivariate integral.
Let $A$ have a strictly positive Hermitian part and $\v{J}$ be a vector of
coefficients that could contain Gra\ss mann variables or complex variables. Then
\begin{equation}
    \label{eq:Field:HS:complexIntegral}
    e^{\v{J}^\dagger A \v{J}} = \det A^{-1}\int_{\mathbb{C}}\prod_{i}\bigg[\frac{\mathrm{d}z_i^\ast\mathrm{d}z_i}{2\pi i}\bigg]e^{-(\v{z}^\dagger A^{-1}\v{z} + \v{z}^\dagger \v{J} + \v{J}^\dagger\v{z})},
\end{equation}
exchanges a quadratic term in $\v{J}$ with an integration over the complex $\v{z}$ variables.
Since $\v{J}$ usually represents some field in a field theory, the new $\v{z}$ is called the auxiliary field or the conjugate
field because of its linear coupling to $\v{J}$.
In the less general case that $A$ is an Hermitian matrix, this formula is proved simply by completing the square, then
diagonalizing $A$ by a unitary transformation and calculating the resulting integrals by the formula 
$\int_{\mathbb{C}}\!\mathrm{d}z^\ast\mathrm{d}z\,e^{-azz^\ast} = 2\pi i /a$. 

From Eq.~\eqref{eq:Field:HS:complexIntegral} we see that
what we have to do to perform the \ac{hs} transformation is first to make a choice for what to interpret as part
of the matrix $A$ and what to interpret as part of $\v{J}$. We then have to check that this definition of $A$ leads to its
Hermitian part having only positive eigenvalues. Finally we need to know an analytical expression for its inverse.
It is usually the first step that is the most difficult since this dictates the low energy excitation a subsequent saddle point
approximation or a stationary phase approximation will produce.
Typically we are interested in transforming a Fermionic interaction potential of the form
\begin{equation}
    \label{eq:Field:HS:typicalInteactionPotential}
    V = \frac{1}{2}\sum_{\alpha\beta\gamma\delta}V_{\alpha\beta\gamma\delta}\xi^\ast_\alpha\xi^\ast_\beta\xi_\delta\xi_\gamma,
\end{equation}
where $\xi_\alpha$ are Gra\ss mann variables, which can be sketched in the way of single-vertex diagram in Figure~\ref{fig:Field:HS:twoBodyInteraction}.
\begin{figure}[h]
    \begin{center}

\feynmandiagram [horizontal = i1 to f1] {
  i1 [particle=$\delta$] -- [fermion] a [dot] -- [anti fermion] i2 [particle=$\gamma$],
  f1 [particle=$\beta$] -- [anti fermion] a -- [fermion] f2 [particle=$\alpha$],
};

    \end{center}
    \caption{Generic two-body interaction.}
    \label{fig:Field:HS:twoBodyInteraction}
\end{figure}
The HS-transformation is classified into being done in a specific \emph{channel} depending on which pair of Gra\ss mann variables are
considered to be part of $\v{J}$ and consequently $\v{J}^\dagger$. The direct channel\footnote{Also known as the density-density channel.} is given by 
the identification $J_i \sim \xi_\alpha^\ast\xi_\gamma$,
the Cooper channel\footnote{Also known as the particle-particle channel.} is defined by the identification $J_i \sim \xi_\delta\xi_\gamma$ while the
exchange channel is given by the identification $J_i \sim \xi_\alpha^\ast\xi_\delta$. Depending on exactly how $\v{J}$ is chosen, the Gaussian
integral in Eq.~\eqref{eq:Field:HS:complexIntegral} might have to be modified. For example in the case of the direct- and exchange-channel, the
exponential argument on the left side will have the form $\v{J}^\trans A\v{J}$ which necessitates the Gaussian integral identity
\begin{equation}
    \label{eq:Field:HS:realIntegral}
    e^{-\frac{1}{2}\v{J}^\trans A\v{J}} = \sqrt{\det A^{-1}}\int_{\mathbb{R}}\!\prod_i\bigg[\frac{\mathrm{d}x_i}{\sqrt{2\pi}}\bigg]e^{-\frac{1}{2}\v{x}^\trans A^{-1}\v{x} - i\v{J}^\trans\v{x}},
\end{equation}
where the auxiliary field $\v{x}$ now is a real conjugate field.


\subsection{Transformation in symmetry channels}

In the Cooper-channel of the \ac{hs} transformation, the complex field $\v{z}$ is conjugate to some combination
of pairs of annihilation operators $\hat{c}_\delta \hat{c}_\gamma$ (or their corresponding Gra\ss mann variables).
The symmetry of the specific combination in turn then determines the symmetry of any low energy field theory obtained
through a subsequent stationary phase approximation. By diagonalizing the interaction potential $\hat{V}$ into its particular
irreducible representations as we will do in Section~\ref{sec:Group:Potential}, then a \ac{hs} transformation
in a specific symmetry channel is done by identifying $\v{J}$ with \ac{ir} basis function combinations of latter operators. 

Let's take the case of a BCS theory of superconductivity where the interaction can be written in terms of
basis functions $d^{(b),m}_{s_1s_2}(\v{k})$ such that the diagonalized interaction takes the form
\begin{equation}
    \label{eq:Field:HS:Symm:diagonalizedV}
    \hat{V} = \sum d^{(b),m}_{s_1s_2}(\v{k})^\ast v_{(b)}d^{(b),m}_{s_1's_2'}(\v{k}') c_{\frac{\v{q}}{2}+\v{k}\,s_1}^\dagger c_{\frac{\v{q}}{2}-\v{k}\,s_2}^\dagger c_{\frac{\v{q}}{2}-\v{k}'s_2'}c_{\frac{\v{q}}{2}+\v{k}'s_1'},
\end{equation}
where $\sum$ indicates the sum over the indices, $\v{k}, \v{k}', \v{q}, s_1, s_2, s_1', s_2', b$ and $m$. Here $b$ specifies the irreducible representation
while $m$ enumerates the representation basis \footnote{For an exposition on the basics of irreducible representations in superconductivity theory see Chapter~\ref{chap:Group}.}.
Identifying
\begin{equation}
    \label{eq:Field:HS:Symm:identification}
    \hat{J}_\v{q}^{(b_m)} = \sum_{\v{k}s_1s_2}d^{(b),m}_{s_1s_2}(\v{k})\hat{c}_{\frac{\v{q}}{2}-\v{k},s_1}\hat{c}_{\frac{\v{q}}{2}+\v{k},s_2},
\end{equation}
the interaction potential is simply written
\begin{equation}
    \label{eq:Field:HS:Symm:HSpreparedV}
    \hat{V} = \sum_{\v{q},b,m}\hat{J}_\v{q}^{(b_m)\,\dagger}v^{(b)}\hat{J}_\v{q}^{(b_m)}.
\end{equation}
In the path-integral representation of the partition function, the annihilation operators become Gra\ss mann variables which we denote
by writing $J$ instead of $\hat{J}$ such that the contribution from the interaction potential results in the exponential
\begin{equation}
    \label{eq:Field:HS:Symm:potentialExp}
    \mathcal{Z}_I = e^{-\int_0^\beta\!\mathrm{d}\tau\sum_{\v{q},b,m}J_\v{q}^{(b_m)\,\dagger}v^{(b)}J_\v{q}^{(b_m)}}.
\end{equation}
Now it is straightforward to use the \ac{hs} formula
\begin{equation}
    \label{eq:Field:HS:Symm:fieldHSTransform}
    e^{\int_0^\beta\!\mathrm{d}\tau\sum_{ij} J_i^\ast A_{ij}J_{j}} = \int\!\mathcal{D}[\eta_i^\ast\eta_i]e^{-\int_0^\beta\!\mathrm{d}\tau\big(\eta_i^\ast A^{-1}_{ij}\eta_j + J_i^\ast\eta_i+J_i\eta_i^\ast\big)},
\end{equation}
which is a path integral version of Eq.~\eqref{eq:Field:HS:complexIntegral}, to transform each pair of irreducible representation basis vectors to
individual conjugate fields. In the notation of Eq.~\eqref{eq:Field:HS:Symm:fieldHSTransform} implicit summation over repeated indices is used and each index $i$ is a collection $i = (b,m,\v{q})$ of indices.
Comparing Eq.~\eqref{eq:Field:HS:Symm:fieldHSTransform} and \eqref{eq:Field:HS:Symm:potentialExp} we gather that
\begin{equation}
    \label{eq:Field:HS:Symm:Matrix}
    A_{ij} = A_{b,m,\v{q};\,b',m',\v{q}'} = -\delta_{\v{q}\v{q}'}\delta_{mm'}\delta_{bb'}v^{(b)},
\end{equation}
which is trivially Hermetic and positive definite provided $v^{(b)}<0$. In this case we say that the irreducible representation $b$ is an
attractive channel. $A$ is in this case also trivially invertible with $A_{ij}^{-1} = -\delta_{ij}/v^{(b)}$.
Writing out all the indices we finally arrive at the \ac{hs} transformation of the interaction potential in individually
attractive symmetry channels
\begin{equation}
    \label{eq:Field:HS:Symm:HSTransformedPotentialExp}
    \mathcal{Z}_I = \int\!\mathcal{D}[\eta_\v{q}^{(b_m)\,\ast}\eta_\v{q}^{(b_m)}]e^{\int_0^\beta\!\mathrm{d}\tau\!\sum\limits_{\v{q}bm}\big[\frac{|\eta_\v{q}^{(b_m)}|}{v^{(b)}} - \big(J_\v{q}^{(b_m)\,\ast}\eta_\v{q}^{(b_m)} + J_\v{q}^{(b_m)}\eta_\v{q}^{(b_m)}\big)\big]},
\end{equation}
where
\begin{equation}
    \label{eq:Field:HS:Symm:JRedef}
    J^{(b_m)}_\v{q} = \sum_{\v{k}\,s_1s_2}d_{s_1s_2}^{(b),m}(\v{k})\xi_{\frac{\v{q}}{2}-\v{k},s_1}\xi_{\frac{\v{q}}{2}+\v{k},s_2}
\end{equation}
in terms of Gra\ss mann variables $\xi$. We note that this derivation does not assume either odd or even basis functions for the
irreducible representations and works just as well for either.

\section{Field theory approximations}

\subsection{Stationary phase and the one loop expansion}

Let $Z$ be the partition function
\begin{equation}
    \label{eq:Field:Stat:partitionFunction}
    Z(l) = \int\!\mathcal{D}[\eta^\ast_\alpha\,\eta_\alpha]\;e^{-lS(\eta^\ast_\alpha, \eta_\alpha)},
\end{equation}
given in terms of bosonic fields $\eta^\ast_\alpha, \eta_\alpha$ and where $l$ is some large parameter $l\gg 1$.
In what is called the stationary phase approximation%
%
\footnote{The name is misleading in the case of the many-particle partition function
since we do not have a strict phase in the exponent but in general a complex function because $\eta_\alpha\in\mathbb{C}$.
Technically it is the method of steepest descent for the case of an exponent with stationary points that is used in this
case.} %
%
of a bosonic field integral, we create an expansion of the free energy around
the field configuration $\{\eta^c_\alpha\}$ where the action $S$ is stationary and a minimum. This configuration
is the main contribution to the integral in Eq.~\eqref{eq:Field:Stat:partitionFunction} since it provides the maximum of
the exponent,
and determines the leading order asymptotic behavior as $l\to\infty$. It also corresponds in a sense to a classical solution
and gives in the case of the Feynman path-integral for the evolution operator of a single particle in an external potential,
the classical Euler-Lagrange equations.
The configuration is found in the path-integral notation by varying the fields in the action such that
\begin{equation}
    \label{eq:Field:Stat:stationaryCondition}
    \frac{\delta S}{\delta \eta_\alpha} = 0\quad\land\quad
    \frac{\delta S}{\delta \eta_\alpha^\ast} = 0.
\end{equation}

As an example, the Hubbard-model with a conventional negative-$U$ Hubbard-potential $U\sum_{\v{r}_i}\hat{n}_{\v{r}_i,\up}\hat{n}_{\v{r}_i,\dn}$,
can be expressed as a bosonic field integral through a \ac{hs} transformation.
The stationary field configuration $\{\eta^c_q(\tau)\}$ is in this case given by a imaginary-time- and spatially-independent
field-configuration $\eta^c$. Assuming such a solution, the action reduces to
\begin{equation}
    \label{eq:Field:Stat:meanFieldHubbard}
    S(\eta^{c\;\ast}, \eta^c) = \frac{\beta N}{U}|\eta^c|^2 - \sum_\v{q}\ln\Big[\Big(1+e^{\beta E_\v{q}}\Big)\Big(1+e^{-\beta E_\v{q}}\Big)\Big],
\end{equation}
where $E_\v{q} = \sqrt{(\epsilon(\v{q})-\mu)^2 + |\eta^c|^2}$, $N$ is the number of hopping sites, $\epsilon(\v{q})$ is the Fourier transformed
kinetic hopping energy, $\mu$ is the chemical potential, and $U$ is the repulsive Hubbard interaction strength. The stationary action condition
in Eq.~\eqref{eq:Field:Stat:stationaryCondition} then yields the equation
\begin{equation}
    \label{eq:Field:Stat:hubbardStationary}
    |\eta^c|\Big(\frac{1}{U} - \frac{1}{N}\sum_\v{q}\frac{1}{2E_\v{q}}\tanh\frac{\beta E_\v{q}}{2}\Big) = 0,
\end{equation}
which has two solutions: one given by $\eta^c = 0$ and one given by setting the terms in the parenthesis equal to $0$. The last non-trivial solution
is a form of the BCS solution and represents the order-parameter in an $s$-wave superconductor. Because of the parameter $\beta$, this
solution will be temperature dependent and disappears at some critical temperature at which the two solutions for $\eta_c$ converge.

The stationary phase expansion is then the expansion resulting from expanding the action around the stationary solution. Setting
$\tilde{\eta}_\alpha = \sqrt{l}(\eta_\alpha-\eta^c_\alpha)$ we get in general the expansion
\begin{equation}
    \label{eq:Field:Stat:stationaryPhaseExpansion}
    \begin{split}
        S(\tilde{\eta}_\alpha,\tilde{\eta}_\alpha^\ast) &= S_c + \frac{1}{l}\bigg[\frac{1}{2!}\frac{\delta^2S}{\delta\eta_\alpha\delta\eta_\beta}\Big|_{\eta_c}\tilde{\eta}_\alpha\tilde{\eta}_\beta\\
        + \frac{1}{2!}&\frac{\delta^2S}{\delta\eta_\alpha^\ast\delta\eta_\beta^\ast}\Big|_{\eta_c}\tilde{\eta}^\ast_\alpha\tilde{\eta}^\ast_\beta
        + \frac{\delta^2S}{\delta\eta_\alpha\delta\eta_\beta^\ast}\Big|_{\eta_c}\tilde{\eta}_\alpha\tilde{\eta}_\beta^\ast\bigg] +  \mathcal{O}\Big(\frac{1}{l\sqrt{l}}\Big)\\
        &= \sum_{n_1+n_2\geq 2}\frac{1}{n_1!n_2!}\frac{\delta^{n_1+n_2}S}{\delta\tilde{\eta}_\alpha^{n_1}\delta\tilde{\eta}_\beta^{\ast\;n_2}}\tilde{\eta}_\alpha^{n_1}\tilde{\eta}_\beta^{\ast\;n_2}\Big|_{\eta_c}\frac{1}{l^{(n_1+n_2)/2}}.
    \end{split}
\end{equation}
We have used implicit summation over repeated indices and on the last line we have assumed that new indices should be introduced and summed for each $n_1$ and $n_2$.
This is simply the multivariate Taylor expansion around $\eta_c$ where we have treated $\eta$ and $\eta^\ast$ as independent variables.

A simpler expansion can be found when the bosonic fields correspond to order parameters in a system close to a phase transition. In this case
we can assume the fields $\eta_\alpha$ to be small in general such that $S$ simply can be expanded about $0$. If the system is fermionic, then
the bosonic field integral of the order-parameters will have resulted from a \ac{hs} transformation, in which case we will have
a contribution to the integral of the form
\begin{equation}
    \label{eq:Field:Stat:determinantContribution}
    \sqrt{\det G^{-1}(\eta_\alpha,\eta_\alpha^\ast)} = e^{\frac{1}{2}\tr\ln G^{-1}},
\end{equation}
where $G^{-1}$ is the result of the integration of a quadratic fermionic action, and will in general depend on the auxiliary bosonic fields
$\eta_\alpha$ and $\eta_\alpha^\ast$ in a non-linear way. Then the matrix $G^{-1}$ can be decomposed in a matrix $G_0^{-1}$ that results purely
from fermionic integration, and a matrix $\phi$ that is dependent on the auxiliary fields $\eta_\alpha$ and importantly vanishes as $\eta_\alpha\to0$,
such that $G^{-1} = G_0^{-1} + \phi$. When the system is close to the phase transition such that the auxiliary fields are small, then this
allows for the expansion of the logarithm in Eq.~\eqref{eq:Field:Stat:determinantContribution} such that
\begin{equation}
    \label{eq:Field:Stat:logarithmExpansion}
    \begin{split}
        \frac{1}{2}\tr\ln(G_0^{-1} + \phi) &= \frac{1}{2}\Big(\tr\ln G_0^{-1} + \tr\ln(1+G_0\phi)\Big)\\
        &= \frac{1}{2}\Big(\tr\ln G_0^{-1} - \sum_{n=1}^\infty\frac{\tr (-G_0\phi)^n}{n}\Big).
    \end{split}
\end{equation}
This is known as the one-loop expansion since in terms of perturbation theory, $G_0$ is the fermionic propagator such that the sum in the
last line of Eq.~\eqref{eq:Field:Stat:logarithmExpansion} corresponds to a series of propagators and interactions that are connected in
a closed loop by the trace.

\subsection{Gradient expansion}

The gradient expansion rests on the assumption that the fields are sufficiently smooth such that progressively higher order derivatives with
respect to the field parameters, are progressively smaller,
\ie we assume 
\begin{equation}
    \label{eq:Field:Grad:gradientComparison}
    l_\alpha^n|\partial_\alpha^n\eta_\alpha| \gg l_\alpha^{n+1}|\partial_\alpha^{n+1}\eta_\alpha|,
\end{equation}
where $l_\alpha$ is some appropriate length scale such as to make $l_\alpha\partial_\alpha$ dimensionless.
In practice this usually means that given a momentum-dependent action density, $S(\eta_{\v{q},\alpha}^\ast,\eta_{\v{q},\alpha};\v{q})$, we assume $q_i$ small compared to the lattice
spacing\footnote{This could be the spacing between hopping sites, \ie stationary ions in an electron model.} $a_i$. Then we expand the
explicit $\v{q}$ dependence in the action density in the small parameters $a_iq_i$ as a Maclaurin series. In the following notation, we will
assume the length parameter $a^i$ is present in the notation $q^i$ where appropriate. The momentum dependence of the fields
themselves should not be expanded as our goal is to have terms of the form $(q^i)^n(\tilde{\eta}_{\v{q},\alpha})^m$. If we then let partial momentum-derivatives
only act on the explicite momentum-dependence in $S$, and neglecting the field dependence in the notation
for $S$, the expanded action can be written as the series
\begin{equation}
    \label{eq:Field:Grad:actionExpansion}
    S(\v{q}) = S(\v{0}) + \frac{\partial S}{\partial q^i}(\v{0})\,q^i + \frac{1}{2!}\frac{\partial^2S}{\partial q^i\partial q^j}(\v{0})\,q^iq^j + \mathcal{O}(q^3).
\end{equation}
Usually, the linear term cancels by symmetry of the underlying lattice. In general it is smart to here check for terms that cancel by considering any internal
momentum sums that may be included in the coefficients.

Terms with products between fields $\eta_{\v{q},\alpha}$ and $q^i$ lead to gradients of the spatially dependent fields $\eta_{\v{R},\alpha}$, which is why the
expansion in Eq.~\eqref{eq:Field:Grad:actionExpansion} can be called a gradient expansion. The spatially dependent fields are defined as the coefficients in the inverse Fourier transform
\begin{equation}
    \label{eq:Field:Grad:inverseFourier}
    \eta_{\v{q},\alpha} = \frac{1}{\sqrt{N}}\sum_\v{R}e^{i\v{q}\cdot\v{R}}\eta_{\alpha}(\v{R}),
\end{equation}
where $N$ is the number of terms in the sums $\sum_\v{q}$ and $\sum_\v{R}$. One way of now obtaining gradients of the spatial fields is to
realize that since $q^i$ is small we can set
\begin{equation}
    \label{eq:Field:Grad:exponential}
    q^i \approx \sin(q^i) = \frac{1}{2i}\Big(e^{iq^i} - e^{-iq^i}\Big).
\end{equation}
With this identification, all the momentum dependence in $S(\v{q})$ exists as phases such that the sum $\sum_\v{q}S(\v{q})$ results in a series of
Kronecker-delta functions which we evaluate by the $\sum_\v{R}$ sums coming from the inverse Fourier transforms in Eq.~\eqref{eq:Field:Grad:inverseFourier}.
Grouping terms of displaced spatial fields, we can identify derivatives, such that
\begin{equation}
    \label{eq:Field:Grad:derivativeId}
    \eta_\alpha(\v{R}+a\hat{e}_i) - \eta_\alpha(\v{R}) \approx a\frac{\partial}{\partial R^i}\eta_\alpha(\v{R}).
\end{equation}
These identifications are justified by going to the continuum limit where $a\to0$.

As an example, consider the sum
\begin{equation}
    \label{eq:Field:Grad:Ex:sum}
    S = \sum_\v{q}K_{\alpha\beta\,ij}q^iq^j\eta_{\v{q},\alpha}^\ast\eta_{\v{q},\beta},
\end{equation}
where there is an implicit summation over repeated indices $i,j,\alpha$ and $\beta$. Fourier transforming the fields according to Eq.~\eqref{eq:Field:Grad:inverseFourier}
and writing $q^i$ as $\sin q^i$, we can group terms such that
\begin{equation}
    \label{eq:Field:Grad:Ex:calculation}
    \begin{split}
        S &= \frac{1}{N}\sum_{\v{R}_1\v{R}_2}K_{\alpha\beta\,ij}\eta_\alpha(\v{R}_1)^\ast\eta_\beta(\v{R}_2)\frac{1}{4}\sum_\v{q}\Big[e^{i\v{q}\cdot(\v{R}_2-\v{R}_1+a\hat{e}_i-a\hat{e}_j)}\\
        + &e^{i\v{q}\cdot(\v{R}_2-\v{R}_1+a\hat{e}_j-a\hat{e}_i)} - e^{i\v{q}\cdot(\v{R}_2-\v{R}_1+a\hat{e}_i+a\hat{e}_j)} - e^{i\v{q}\cdot(\v{R}_2-\v{R}_1-a\hat{e}_i-a\hat{e}_j)}\Big]\\
        &= \sum_{\v{R}}K_{\alpha\beta\,ij}a^2\frac{\partial}{\partial R_i}\eta_\alpha(\v{R})^\ast\frac{\partial}{\partial R_j}\eta_\beta(\v{R}).
    \end{split}
\end{equation}

A more conventional way of converting to gradients, is to use integration by parts. Then the product rule of partial derivatives is used to obtain
\begin{equation}
    \label{eq:Field:Grad:productRuleDerivative}
    q^j\eta_\alpha(\v{R})e^{i\v{q}\cdot\v{R}} = i\frac{\partial\eta_\alpha}{\partial R^i}e^{i\v{q}\cdot\v{R}} - i\frac{\partial}{\partial R^i}\Big[\eta_\alpha(\v{R})e^{i\v{q}\cdot\v{R}}\Big].
\end{equation}
Summing on both sides and arguing that the boundary term vanishes because $\eta_\alpha(\v{R})\to0$ as $R^i\to\infty$, then
\begin{equation}
    \label{eq:Field:Grad:partialIntegraion}
    \sum_\v{R}q^j\eta_\alpha(\v{R})e^{i\v{q}\cdot\v{R}} = i\sum_\v{R}e^{i\v{q}\cdot\v{R}}\nabla_i\eta_\alpha(\v{R}).
\end{equation}
